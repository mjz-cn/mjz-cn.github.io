{"pages":[{"title":"404 Not Found：该页无法显示","text":"Not Found","link":"/404.html"},{"title":"关于","text":"人在北京，毕业于湖南大学、华中科技大学，从事互联网行业。 期望包荒山，开果园，种果树，葡萄架下写代码。 邮箱：369806726@qq.com","link":"/about/index.html"},{"title":"书单","text":"现代操作系统","link":"/books/index.html"},{"title":"友情链接","text":"","link":"/links/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"},{"title":"Repositories","text":"","link":"/repository/index.html"}],"posts":[{"title":"Flink源码分析-State","text":"Flink中的容错，一致性语义都是靠State来实现的。 State需要结合Checkpoint，Snapshot才能发挥作用。 State 可以按照维度进行划分： 类型： Keyed State Operator 数据组织格式： Managed (List Value Map) Raw (用户自定义的格式) State Backend是用于存储State的, 当前有三种存储： MemoryStateBackend (仅在测试时用) FsStateBackend RocksDBStateBackend 获取一个状态： 当从StateBackend获取一个状态时，会首先根据状态名称来判断其是否存在，如果存在就取出来，不存在就创建新的。 当Checkpoint触发算子Snapshot的时候，会保存算子的State到StateBackend。 CheckpointingOperation.executeCheckpointing 当一个task开始快照的时候，会调用这个方法 123456789101112131415161718192021222324252627public void executeCheckpointing() throws Exception { ... try { // 对当前任务关联的所有算子执行快照 for (StreamOperator&lt;?&gt; op : allOperators) { // 这个方法会生成每种状态的快照Future，例如：keyedStateManagedFuture operatorStateManagedFuture // 生成写入StateBackend的RunnableFutre，都保存在operatorSnapshotsInProgress中 checkpointStreamOperator(op); } // 真正的将状态写入到StateBackend中 AsyncCheckpointRunnable asyncCheckpointRunnable = new AsyncCheckpointRunnable( owner, operatorSnapshotsInProgress, // 非常关键的参数 checkpointMetaData, checkpointMetrics, startAsyncPartNano); owner.cancelables.registerCloseable(asyncCheckpointRunnable); // 异步执行写入操作 owner.asyncOperationsThreadPool.execute(asyncCheckpointRunnable); ... } catch (Exception ex) { ... }} AbstractStreamOperator.snapshotState 1234567891011121314151617181920212223242526272829303132333435 // factory生成真正执行保存State的OutputStreampublic final OperatorSnapshotFutures snapshotState(long checkpointId, long timestamp, CheckpointOptions checkpointOptions, CheckpointStreamFactory factory) throws Exception { OperatorSnapshotFutures snapshotInProgress = new OperatorSnapshotFutures(); // 保存有checkpointId，timestamp StateSnapshotContextSynchronousImpl snapshotContext = ...; try { // 调用 AbstractUdfStreamOperator.snapshotState // 会调用userFunction的snapshotState() // 如果用户实现的是ListCheckpointed, 就会向OperatorStateBackend申请一个PartitionableListState并保存在OperatorStateBackend.registeredOperatorStates中 snapshotState(snapshotContext); // 生成Raw State RunnableFuture，这个future中只有keyedStateCheckpointOutputStream，会在userFunction中执行out.write snapshotInProgress.setKeyedStateRawFuture(snapshotContext.getKeyedStateStreamFuture()); snapshotInProgress.setOperatorStateRawFuture(snapshotContext.getOperatorStateStreamFuture()); if (null != operatorStateBackend) { // (DefaultOperatorStateBackendSnapshotStrategy) operatorStateBackend.snapshot会生成一个将managed state写入StateBackend的runnable // 如果为同步快照，此方法就会变成一个同步操作，直接执行AsyncSnapshotCallable snapshotInProgress.setOperatorStateManagedFuture( operatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions)); } if (null != keyedStateBackend) { snapshotInProgress.setKeyedStateManagedFuture( keyedStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions)); } } catch (Exception snapshotException) { ... } return snapshotInProgress; } StreamTask.asyncOperationsThreadPool执行AsyncCheckpointRunnable.run来异步的将状态持久化到StateBackend，并且向JobMaster发送Ack,表示checkpoint操作完成 12345678910111213141516171819202122232425262728293031323334353637383940414243 FileSystemSafetyNet.initializeSafetyNetForThread();try { // 快照结果，并且发送给JobMaster TaskStateSnapshot jobManagerTaskOperatorSubtaskStates = ...; TaskStateSnapshot localTaskOperatorSubtaskStates = ...; for (Map.Entry&lt;OperatorID, OperatorSnapshotFutures&gt; entry : operatorSnapshotsInProgress.entrySet()) { OperatorID operatorID = entry.getKey(); // 异步快照Future集合 OperatorSnapshotFutures snapshotInProgress = entry.getValue(); // 判断Future是否已经（同步执行，与userFunction同一个线程）执行过， // 如果没有就在当前线程执行, 相当于异步执行快照 // OperatorSnapshotFinalizer.jobManagerOwnedState 保存所有类型状态的结果 OperatorSnapshotFinalizer finalizedSnapshots = new OperatorSnapshotFinalizer(snapshotInProgress); jobManagerTaskOperatorSubtaskStates.putSubtaskStateByOperatorID( operatorID, finalizedSnapshots.getJobManagerOwnedState()); localTaskOperatorSubtaskStates.putSubtaskStateByOperatorID( operatorID, finalizedSnapshots.getTaskLocalState()); } ... // 快照完成 if (asyncCheckpointState.compareAndSet(CheckpointingOperation.AsyncCheckpointState.RUNNING, CheckpointingOperation.AsyncCheckpointState.COMPLETED)) { // 向JobMaster发送快照结果， reportCompletedSnapshotStates( jobManagerTaskOperatorSubtaskStates, localTaskOperatorSubtaskStates, asyncDurationMillis); } else { ... } ...} DefaultOperatorStateBackendSnapshotStrategy.snapshot被AbstractStreamOperator.snapshotState调用 获取用户注册的算子状态和广播状态 生成AsyncSnapshotCallable instance 如果 asynchronousSnapshots == true，直接执行 AsyncSnapshotCallable 返回 FutureTask AsyncCheckpointRunnable.run在初始化OperatorSnapshotFinalizer的时候，会挨个调用OperatorSnapshotFutures中的RunnableFuture keyedStateManagedFuture关联的是HeapSnapshotStrategy.AsyncSnapshotCallable operatorStateManagedFuture关联的是DefaultOperatorStateBackendSnapshotStrategy.AsyncSnapshotCallable 算子状态持久化过程DefaultOperatorStateBackendSnapshotStrategy.AsyncSnapshotCallable 123456789101112131415161718192021222324252627282930313233343536// 创建OutputStream, 输入到文件或者内存CheckpointStreamFactory.CheckpointStateOutputStream localOut = streamFactory.createCheckpointStateOutputStream(CheckpointedStateScope.EXCLUSIVE);snapshotCloseableRegistry.registerCloseable(localOut);// get the registered operator state infos ...List&lt;StateMetaInfoSnapshot&gt; operatorMetaInfoSnapshots = ...;// ... get the registered broadcast operator state infos ...List&lt;StateMetaInfoSnapshot&gt; broadcastMetaInfoSnapshots = ...;// ... write them all in the checkpoint stream ...DataOutputView dov = new DataOutputViewStreamWrapper(localOut);OperatorBackendSerializationProxy backendSerializationProxy = new OperatorBackendSerializationProxy(operatorMetaInfoSnapshots, broadcastMetaInfoSnapshots);// 实际的写入操作backendSerializationProxy.write(dov);// we put BOTH normal and broadcast state metadata herefinal Map&lt;String, OperatorStateHandle.StateMetaInfo&gt; writtenStatesMetaData = new HashMap&lt;&gt;(initialMapCapacity);// ... and, finally, create the state handle.OperatorStateHandle retValue = null;if (snapshotCloseableRegistry.unregisterCloseable(localOut)) { // 关闭流并执行flush StreamStateHandle stateHandle = localOut.closeAndGetHandle(); if (stateHandle != null) { retValue = new OperatorStreamStateHandle(writtenStatesMetaData, stateHandle); } // 会发送给JobMaster return SnapshotResult.of(retValue);} else { throw new IOException(&quot;Stream was already unregistered.&quot;);}","link":"/2019/12/02/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-State/"},{"title":"Flink源码分析-Watermark","text":"简介窗口机制是Flink流处理的核心，它将无限的流元素分割成有限的窗口。当一个窗口不再增加新元素时，就可以对这个窗口中的所有元素执行计算逻辑。而判断窗口不会再增加新元素的方式有：时间（Watermark）、计数、自定义。其中Watermark代表事件发生时的时间戳或者Flink收到事件时的时间戳。 Watermark可以分为三个部分来理解： 来源 Watermark的来源，它是如何产生的？在不同的配置下，它的产生方式有什么不同？ 流转 当产生了一个Watermark时，它流转到WindowOperator时都会经过哪些过程？ 触发 当WindowOperator收到Watermark时，如何处理？这个过程是通过触发器来处理的，用户可以自定义触发器，也可以使用系统提供的触发器，可参考下EventTimeTrigger的实现。本章不再叙述 注意：Watermark如果不能及时产生或者流转到WindowOperator就会造成窗口计算的延迟甚至导致窗口一直不会被触发。 Watermark来源Watermark的来源有三种： AutomaticWatermarkContext自动以系统时间来做为Watermark，只能发生在SourceTask 用户主动调用SourceContext.emitWark()，只能发生在SourceTask TimestampsAndPeriodicWatermarksOperator从elemet中提取出Watermark，通过定时任务发射Watermark，这一步骤可以发生在拓扑图中的任意一个环节。 SourceContext的初始化首先看下SourceStream算子的启动过程： SourceContext的作用主要是用于发射elements，也有可能发射Watermark，从Source算子启动的流程图中可以看出一共它有三种类型。SourceStream在启动的时候会根据配置的时间类型来选择对应的SourceContext，它的直接子类只有两个WatermarkContext 和 NonTimestampContext。 WatermarkContext处理与Watermark相关的操作。也要维护当前Stream的StreamStatus，使stream的状态能正确传递给下游。这样做是因为流的状态能会影响下游到对Watermark的处理。下游处理StreamStatus和Watermark的相关逻辑可以参考StatusWatermarkValve，它会决定元素是否继续传递给下游。 WatermarkContext 又有两个子类： AutomaticWatermarkContext 对应的配置是IngestionTime。它会启动一个定时任务，以固定的时间间隔从系统中提取系统时间作为Watermark发射到下游。 ManualWatermarkContext 对应的配置是EventTime。用户通过它提供的方法可以主动发射Watermark。 NonTimestampContext发射的所有元素都不会携带时间戳，并且它也不不能发射Watermark。对应的配置是ProcessingTime，它不会产生Watermark，而是由WindowOperator在收到新元素时，直接根据当前系统时间判断是否要触发计算逻辑。 AutomaticWatermarkContext逻辑注册定时任务 123456789private AutomaticWatermarkContext(...) { super(timeService, checkpointLock, streamStatusMaintainer, idleTimeout); // 初始化过程 ... // 注册一个定时发射任务 long now = this.timeService.getCurrentProcessingTime(); this.nextWatermarkTimer = this.timeService.registerTimer(now + watermarkInterval, new WatermarkEmittingTask(this.timeService, checkpointLock, output));} 当收到一个元素时，会判断是否要发射Watermark 12345678910111213141516protected void processAndCollect(T element) { // 取系统时间做为Watermark lastRecordTime = this.timeService.getCurrentProcessingTime(); // 发射元素 output.collect(reuse.replace(element, lastRecordTime)); // this is to avoid lock contention in the lockingObject by // sending the watermark before the firing of the watermark // emission task. if (lastRecordTime &gt; nextWatermarkTime) { // in case we jumped some watermarks, recompute the next watermark time final long watermarkTime = lastRecordTime - (lastRecordTime % watermarkInterval); nextWatermarkTime = watermarkTime + watermarkInterval; output.emitWatermark(new Watermark(watermarkTime)); }} 定时任务 WatermarkEmittingTask，它是AutomaticWatermarkContext的内部类。 123456789101112131415161718192021222324252627private class WatermarkEmittingTask implements ProcessingTimeCallback { ... @Override public void onProcessingTime(long timestamp) { final long currentTime = timeService.getCurrentProcessingTime(); synchronized (lock) { // we should continue to automatically emit watermarks if we are active if (streamStatusMaintainer.getStreamStatus().isActive()) { if (idleTimeout != -1 &amp;&amp; currentTime - lastRecordTime &gt; idleTimeout) { // 检测stream的状态，如果超过一定时间没有元素进来，就会将此stream置位空闲状态从而影响下有对watermark的处理 markAsTemporarilyIdle(); cancelNextIdleDetectionTask(); } else if (currentTime &gt; nextWatermarkTime) { // 发射 final long watermarkTime = currentTime - (currentTime % watermarkInterval); output.emitWatermark(new Watermark(watermarkTime)); nextWatermarkTime = watermarkTime + watermarkInterval; } } } long nextWatermark = currentTime + watermarkInterval; // 注册下一次的定时任务 nextWatermarkTimer = this.timeService.registerTimer(nextWatermark, new WatermarkEmittingTask(this.timeService, lock, output)); }} TimestampsAndPeriodicWatermarksOperator是一个StreamOperator，可以在拓扑图中的任一环节。当job的时间类型为EventTime时，在拓扑图中需要增加这个算子。 123456789101112131415161718192021222324252627282930313233343536373839404142public class TimestampsAndPeriodicWatermarksOperator { ... public void open() throws Exception { ... currentWatermark = Long.MIN_VALUE; watermarkInterval = getExecutionConfig().getAutoWatermarkInterval(); // 如果间隔大于0表示开启定时发射watermark任务 if (watermarkInterval &gt; 0) { long now = getProcessingTimeService().getCurrentProcessingTime(); getProcessingTimeService().registerTimer(now + watermarkInterval, this); } } public void processElement(StreamRecord&lt;T&gt; element) throws Exception { final long newTimestamp = userFunction.extractTimestamp(element.getValue(), element.hasTimestamp() ? element.getTimestamp() : Long.MIN_VALUE); // 将element的时间戳替换为从elemnt中提取的时间戳 output.collect(element.replace(element.getValue(), newTimestamp)); } public void onProcessingTime(long timestamp) throws Exception { // 从elemnt中提取时间戳 // 这里有一个问题，如果上游一直没有发送元素并且也没切换StreamStatus就会导致watermark阻塞到下游，不能再向前传递 Watermark newWatermark = userFunction.getCurrentWatermark(); if (newWatermark != null &amp;&amp; newWatermark.getTimestamp() &gt; currentWatermark) { currentWatermark = newWatermark.getTimestamp(); output.emitWatermark(newWatermark); } long now = getProcessingTimeService().getCurrentProcessingTime(); getProcessingTimeService().registerTimer(now + watermarkInterval, this); } public void processWatermark(Watermark mark) throws Exception { // 忽略上游传递的watermarks，仅传递当前算子产生的时间戳。除了代表流结束的watermark if (mark.getTimestamp() == Long.MAX_VALUE &amp;&amp; currentWatermark != Long.MAX_VALUE) { currentWatermark = Long.MAX_VALUE; output.emitWatermark(mark); } }} 下游对Watermark的处理下游接收上游数据的流程图： 从流程图中可以很清楚的看出来，Watermark、StreamStatus都是交给StatusWatermarkValve来进行处理。它相当于一个阀门，当收到水位线或者流的状态的时候判断是否满足相应的条件，如果满足才会再继续发射到下游。 首先看下阀门的一个内部类InputChannelStatus，这个类非常重要，它封装了上游InputChannel的状态，在计算过程中都会用到。 12345678protected static class InputChannelStatus { // 最近一个水位线 long watermark; // 上游状态，Active 或者 Idle StreamStatus streamStatus; // 标示上游的水位线是否对齐，只有对齐时才会使用水位线 boolean isWatermarkAligned;} 处理水位线的大致流程如下： 这个流程复杂，重点看下如何找最小的水位线，这一步骤决定是否发射新的水位线到下游。 12345678910111213141516171819private void findAndOutputNewMinWatermarkAcrossAlignedChannels() { long newMinWatermark = Long.MAX_VALUE; boolean hasAlignedChannels = false; // 找最小的水位线 for (InputChannelStatus channelStatus : channelStatuses) { if (channelStatus.isWatermarkAligned) { hasAlignedChannels = true; newMinWatermark = Math.min(channelStatus.watermark, newMinWatermark); } } // 由于所有的InputChannelStatus默认都是激活、对齐，且水位线=Long.MIN_VALUE。 // 如果某个上游（InputChannel）一直没有发送新的水位线且也没有切换状态。 // 这样newMinWatermark的值一直都是Long.MIN_VALUE，从而不会发射新的watermark // 上述情况在EventTime时，可能会发生。 if (hasAlignedChannels &amp;&amp; newMinWatermark &gt; lastOutputWatermark) { lastOutputWatermark = newMinWatermark; outputHandler.handleWatermark(new Watermark(lastOutputWatermark)); }} 处理StreamStatus的过程如下： 123456789101112131415161718192021222324252627282930313233public void inputStreamStatus(StreamStatus streamStatus, int channelIndex) { if (streamStatus.isIdle() &amp;&amp; channelStatuses[channelIndex].streamStatus.isActive()) { // 处理active -&gt; idle的切换 channelStatuses[channelIndex].streamStatus = StreamStatus.IDLE; // 当channel位idle状态时，设置为非对齐状态，不会影响找newMinWatermark channelStatuses[channelIndex].isWatermarkAligned = false; // 如果所有的上游都为idle，就会给下游发送idle，并且当前的算子状态也置为idle if (!InputChannelStatus.hasActiveChannels(channelStatuses)) { if (channelStatuses[channelIndex].watermark == lastOutputWatermark) { findAndOutputMaxWatermarkAcrossAllChannels(); } lastOutputStreamStatus = StreamStatus.IDLE; outputHandler.handleStreamStatus(lastOutputStreamStatus); } else if (channelStatuses[channelIndex].watermark == lastOutputWatermark) { // 重新找newMinWatermark，且有可能 newMinWatermark &gt; channelStatuses[channelIndex].watermark findAndOutputNewMinWatermarkAcrossAlignedChannels(); } } else if (streamStatus.isActive() &amp;&amp; channelStatuses[channelIndex].streamStatus.isIdle()) { // 处理idle -&gt; active的切换 channelStatuses[channelIndex].streamStatus = StreamStatus.ACTIVE; // 对齐 if (channelStatuses[channelIndex].watermark &gt;= lastOutputWatermark) { channelStatuses[channelIndex].isWatermarkAligned = true; } // 重新激活当前算子 if (lastOutputStreamStatus.isIdle()) { lastOutputStreamStatus = StreamStatus.ACTIVE; outputHandler.handleStreamStatus(lastOutputStreamStatus); } }}","link":"/2020/03/02/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-Watermark%E5%8E%9F%E7%90%86/"},{"title":"Flink源码分析-Checkpoint","text":"Keyword Checkpoint Snapshot Barrier State Savepoint CheckpointCoordinator定时checkpointCheckpointCoordinator会启动一个定时任务触发checkpoint JobMaster启动时，会调用Execution.scheduleForExecution 切换到Running状态时，启动Checkpoint定时任务 当CheckpointCoordinatorDeActivator.jobStatusChanges被调用时，CheckpointCoordinator.startCheckpointScheduler开启CheckpointCoordinator定时任务 CheckpointCoordinatorDeActivator.triggerCheckpoint会被定时触发 checkpointID = checkpointIdCounter.getAndIncrement();递增生成ID 给checkpoint设置超时时间, 超时之后退出此次checkpoint，取消之前的调度任务，重新调度checkpoint 12345678910111213141516 // schedule the timer that will clean up the expired checkpointsfinal Runnable canceller = () -&gt; { synchronized (lock) { // only do the work if the checkpoint is not discarded anyways // note that checkpoint completion discards the pending checkpoint object if (!checkpoint.isDiscarded()) { LOG.info(&quot;Checkpoint {} of job {} expired before completing.&quot;, checkpointID, job); failPendingCheckpoint(checkpoint, CheckpointFailureReason.CHECKPOINT_EXPIRED); pendingCheckpoints.remove(checkpointID); rememberRecentCheckpointId(checkpointID); // 取消之前的调度任务，重新调度checkpoint triggerQueuedRequests(); } }}; 触发source算子checkpoint 12345678// send the messages to the tasks that trigger their checkpointfor (Execution execution: executions) { if (props.isSynchronous()) { execution.triggerSynchronousSavepoint(checkpointID, timestamp, checkpointOptions, advanceToEndOfTime); } else { execution.triggerCheckpoint(checkpointID, timestamp, checkpointOptions); }} 通过Rpc TaskExecutorGateway.triggerCheckpoint 执行 StreamTask.performCheckpoint 并且生成初始的CheckpointBarrier 下游在收到CheckpointBarrier后，会执行StreamTask.triggerCheckpointOnBarrier执行StreamTask.performCheckpoint 并且继续向下游发送CheckpointBarrier StreamTask.performCheckpoint最终会通过CheckpointingOperation.executeCheckpointing来执行实际的checkpoint 下游算子收到CheckpointBarrier的过程 StreamOneInputProcessor.processInput 获取下一个StreamElement StreamTaskNetworkInput.pollNextNullable 调用 CheckpointedInputGate.pollNext来获取下一个BufferOrEvent CheckpointedInputGate.pollNext，如果当前的buffer不为空，从buffer取BufferOrEvent；否则从InputGate取 判断当前BufferOrEvent是否处于阻塞状态，如果处于阻塞状态，将BufferOrEvent添加到buffer中 如果为BufferOrEvent为Buffer直接返回，buffer表示上游发的正常内容 如果为CheckpointBarrier, 调用CheckpointBarrierAligner.processBarrier处理checkpoint事件 CheckpointBarrierAligner.processBarrier中，如果当前算子的上游只有一个的话，会直接触发notifyCheckpoint 对barrier事件的处理又可以分为以下几种情况： barrier为新的事件，则会开启对应通道的对齐beginNewAlignment操作，此操作会阻塞对应channel的元素处理逻辑 如果当前算子已经收到的有barrier，判断barrier是否等于算子当前正在处理的currentCheckpointId, 如果相等，则执行阻塞操作onBarrier 如果barrierId &gt; currentCheckpointId，说明currentCheckpointId出现异常。上报异常，并且充值算子checkpoint状态 如果barrierId &lt; currentCheckpointId 不做任何处理 如果收到所有的barrier，开始执行CheckpointBarrierHandler.notifyCheckpoint StreamTask.triggerCheckpointOnBarrier StreamTask.performCheckpoint 继续向下游发送operatorChain.broadcastCheckpointBarrier checkpoint barrier广播，执行快照checkpointState() Snapshot操作 CheckpointingOperation.executeCheckpointing 会对当前StreamTask相关联的所有Operator挨个执行CheckpointingOperation.checkpointStreamOperator AbstractStreamOperator.snapshotState会对以下几种类型的状态都执行快照 用户算子状态 managed keyedState raw keystate snapshotContext.getKeyedStateStreamFuture() managed operator state raw operator state snapshotContext.getOperatorStateStreamFuture() AbstractUdfStreamOperator.snapshotState调用StreamingFunctionUtils.snapshotFunctionState对UserFunction执行快照操作 StreamingFunctionUtils.trySnapshotFunctionState中会实际调用自定义的snapshotState方法 自定义的方法只有实现了CheckpointedFunction 或者 ListCheckpointed 才能执行snapshot操作 Task发送Checkpoint AckTask在完成当前的快照之后会向JobMaster的CheckpointCoordinator回复一条Ack消息 StreamTask.executeCheckpointing 调用当前Task的所有Operator的snapshotState方法 执行异步操作AsyncCheckpointRunnable 等待snapshot结果 完成之后，调用reportCompletedSnapshotStates，获取 TaskStateManager taskStateManager = owner.getEnvironment().getTaskStateManager(); taskStateManager.reportTaskStateSnapshots 存储localStateStore.storeLocalState存储localstate RpcCheckpointResponder.acknowledgeCheckpoint通过Rpc向CheckpointCoordinator发送ack消息表示本次checkpoint操作完成 CheckpointCoordinator响应Ack消息 CheckpointCoordinatorGateway的实现类为JobMaster task在完成checkpoint之后通过rpc方式调用JobMaster.acknowledgeCheckpoint发送ack JobMaster调用SchedulerNG.acknowledgeCheckpoint ExecutionGraph.getCheckpointCoordinator()获取对应的CheckpointCoordinator 将CheckpointCoordinator对这条消息的处理放入到一个线程池中进行处理 1234567ioExecutor.execute(() -&gt; { try { checkpointCoordinator.receiveAcknowledgeMessage(ackMessage, taskManagerLocationInfo); } catch (Throwable t) { log.warn(&quot;Error while processing checkpoint acknowledgement message&quot;, t); } }); CheckpointCoordinator会对ack消息做以下校验 PendingCheckpoint checkpoint = pendingCheckpoints.get(checkpointId); checkpoint存在并且没被丢弃，执行正常的处理逻辑checkpoint.acknowledgeTask 存在但是被丢弃，抛出异常 不存在，执行丢弃discardSubtaskState() PendingCheckpoint分别保存了notYetAcknowledgedTasks和acknowledgedTasks PendingCheckpoint.acknowledgeTask处理收到的ack消息，上报checkpoint统计消息。并且更新内部状态 notYetAcknowledgedTasks未收到ack的所有sub task acknowledgedTasks已收到的task operatorStatesOperator状态 PendingCheckpoint.acknowledgeTask根据checkpointId返回三种状态，SUCCESS DUPLICATE UNKNOWN DISCARDED 当ack为SUCCESS时 12345// 收到全部的ack消息if (checkpoint.isFullyAcknowledged()) { // 执行完成操作 completePendingCheckpoint(checkpoint);} CompletedCheckpoint completedCheckpoint = pendingCheckpoint.finalizeCheckpoint(); checkpoint在切换状态时 12345678910111213141516171819202122232425262728293031try { // 生成savepoint，包含了本次checkpoint的所有状态 // write out the metadata final Savepoint savepoint = new SavepointV2(checkpointId, operatorStates.values(), masterState); final CompletedCheckpointStorageLocation finalizedLocation; // 写到statebackend try (CheckpointMetadataOutputStream out = targetLocation.createMetadataOutputStream()) { Checkpoints.storeCheckpointMetadata(savepoint, out); finalizedLocation = out.closeAndFinalizeCheckpoint(); } CompletedCheckpoint completed = new CompletedCheckpoint(); ... // 切换状态 // mark this pending checkpoint as disposed, but do NOT drop the state dispose(false); return completed;} ``` 3. 调用`triggerQueuedRequests()`来触发下一次的checkpoint4. `dropSubsumedCheckpoints(checkpointId);` 丢弃之前的pending状态的checkpoint5. Execution.TaskManagerGateway.notifyCheckpointComplete通知所有Task本次checkpoint完成 ``` for (ExecutionVertex ev : tasksToCommitTo) { Execution ee = ev.getCurrentExecutionAttempt(); if (ee != null) { ee.notifyCheckpointComplete(checkpointId, timestamp); } } 监听Checkpoint完成当operator收到checkpoint完成事件之后，就可以做一些额外操作，例如： Kafka提交offset TwoPhaseCommitSinkFunction","link":"/2019/11/27/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-Checkpoint/"},{"title":"Flink源码分析-Graph","text":"Flink以DAG的方式来执行程序，它会根据用户的代码生成三个Graph，但我认为实际上还有一个Graph，就是用户的程序直接映射出来的。 Plan ProgramGraph StreamGraph JobGraph ExecutionGraph JobGraph12345678910111213141516171819public static boolean isChainable(StreamEdge edge, StreamGraph streamGraph) { StreamNode upStreamVertex = streamGraph.getSourceVertex(edge); StreamNode downStreamVertex = streamGraph.getTargetVertex(edge); StreamOperatorFactory&lt;?&gt; headOperator = upStreamVertex.getOperatorFactory(); StreamOperatorFactory&lt;?&gt; outOperator = downStreamVertex.getOperatorFactory(); return downStreamVertex.getInEdges().size() == 1 &amp;&amp; outOperator != null &amp;&amp; headOperator != null &amp;&amp; upStreamVertex.isSameSlotSharingGroup(downStreamVertex) &amp;&amp; outOperator.getChainingStrategy() == ChainingStrategy.ALWAYS &amp;&amp; (headOperator.getChainingStrategy() == ChainingStrategy.HEAD || headOperator.getChainingStrategy() == ChainingStrategy.ALWAYS) &amp;&amp; (edge.getPartitioner() instanceof ForwardPartitioner) &amp;&amp; edge.getShuffleMode() != ShuffleMode.BATCH &amp;&amp; upStreamVertex.getParallelism() == downStreamVertex.getParallelism() &amp;&amp; streamGraph.isChainingEnabled(); } 判断两个算子是否能串联到一起 图允许链接 下游节点只有一个上游节点 有同样的槽共享组 上下游并行度一致 边的shuffle模式不能为batch 边的分区为Forward分区器 下游的链接策略为always 上游的链接策略为 HEAD 或者 always","link":"/2019/11/29/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-Graph/"},{"title":"Flink源码分析-任务部署","text":"Flink Job有三种部署模式： LAZY_FROM_SOURCES 仅当Task的上游都产生数据之后，才会真正的部署Task。 LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST 与LAZY_FROM_SOURCES逻辑基本一致。不同的是在申请Slot的时候有超时时间限制。 EAGER ExecutionGraph在被调度时会将所有Task一次性到各个TaskManager。 用户在程序从提交到真正执行大概需要以下几个步骤： 生成StreamGraph StreamGraph转换成JobGraph 将JobGraph提交到集群的Dispatcher组件 Dispatcher持久化存储JobGraph，在稍后的部署Task过程中会用到 Dispatcher启动JobManager，将JobGraph转换成ExecutionGraph JobManager启动时，会触发ExecutionGraph的部署 不同的部署模式会导致ExecutionGraph申请资源的方式不一样，流处理默认是EAGER 遍历所有的ExecutionVertex申请相应的Slot 造Slot都申请完毕之后，开始部署Execution 构造TaskDeploymentDescriptor，这个描述符包含了部署一个Task需要的所有信息。JobInformation和TaskInformation都会序列化成二进制数据，如果二进制数据过大会通过BlobStore存储。 获取Slot对应的TaskManagerGateway，通过Rpc调用将Task描述符提交到TaskManager执行 TaskManager在收到Task描述符之后，首先会去加载之前JobInformation和TaskInformation序列化后的数据。如果它们的二进制数据是存储在Blob中，则会通过BlobService加载对应的文件，然后反序列化成对应的class对象。此Task需要执行的代码包含在TaskInformation中。 生成Task对象并且开启Task的执行线程 Task在启动后，首先会进行初始化的动作，invokable = loadAndInstantiateInvokable(userCodeClassLoader, nameOfInvokableClass, env);实例化相应的Task(StreamTask)，这一步会加载用户的代码。","link":"/2019/12/09/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E4%BB%BB%E5%8A%A1%E9%83%A8%E7%BD%B2/"},{"title":"Flink源码分析-数据流转","text":"将一条消息从被Flink job消费到最后被sink下来的整个过程划分成两部分，算子的逻辑处理、task之间的消息传递。其中算子的逻辑处理需要用户参与，task之间的消息传递一般是不需要用户参与的，但是了解其实现过程，对理解Flink的原理是非常有帮助的。 概念在Flink job被执行前，会生成StreamGraph、JobGraph、ExecutionGraph。 StreamPartitioner 流分区器 分区器决定上游算子的子任务以何种方式将消息发送给下游算子的子任务，也就是如何对数据流进行物理分区。流分区器的类型有： BROADCAST 上游的消息以广播的形式发送给所有的下游 REBALANCE 负载均衡的方式发送 SHUFFLE 随机选择下游 CUSTOM 用户自定义分区方式 HASH 用户自定义分区方式，与CUSTOM区别是专用于KeydStream FORWARD 点到点进行发送，这种分区方式下消息只是在进程内传递。 中间数据集 数据从开始到结束大概会经过以下过程： SourceFunction 产出 Record 通过Partitioner 为此record选择要发送到哪个channel 发送到ResultPartition 选择ResultSubpartition 通知JobManager JM部署下游 下游Inputchannel 请求 ResultSubpartition 参考: Data exchange between tasks Task之间数据传输","link":"/2019/11/21/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BD%AC/"},{"title":"Flink源码分析-集群部署","text":"分析Flink程序的启动过程，有助于理解和把握Flink是如何启动程序，集群和单机运行的区别，当出错时如何恢复重启，如何分配资源。 Demo先看一个简单的stream demo 1234567891011public static void main(String[] args) throws Exception { // 获取执行环境，如果是在本地模式下被调用，返回的是LocalStreamEnvironment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 构建DAG，DAG是由多个 Operator(Transformation(UserFunction)) 组成 DataStream&lt;Integer&gt; source = env.fromElements(1, 2, 3, 4, 5); source.map((MapFunction&lt;Integer, Integer&gt;) value -&gt; value).print(); // 执行 env.execute(); } 执行环境Flink程序有两种执行环境： 集群 通过Client将程序提交到其它集群执行 本地 通过Flink程序的main方法执行，这种方式适用于本地开发调试。 注意：Flink程序通过集群或者本地方式运行时，都会调用main方法。但是集群中调用和本地调用的方式不一样。 关键概念组件Flink Cluster由一个Flink Master和多个Task Manager组成，这几个组件既可以是单独的进程，也可以都在一个进程。Flink集群有两种模式： session 可以同时执行多个Flink Job，集群的生命周期不依赖Job的生命周期，这种状态的集群被称为Flink Session Cluster。 job 集群只执行一个Flink Job，当Job状态结束之后，集群也会结束，这种状态的集群被称为Flink Session Application。 Flink Job MasterFlink集群的Master节点，由三部分组成Dispatcher、 ResourceManager、 JobManager组成。Master节点中Dispatcher、 ResourceManager的实例只会有一个，JobManager可能会有多个实例。 集群运行时的几个关键类ExecutionEnvironment 执行环境是Flink程序的运行上下文。批处理的执行环境是：ExecutionEnvironment; 流处理的执行环境是：StreamExecutionEnvironment。Flink程序既能在本地的JVM中运行使用Local...ExecutionEnvironment;发送到远程运行则使用Remote...ExecutionEnvironment; Dispatcher 接收任务的提交并且会将jobgraph持久化存储，之后在集群重启的时候使用 执行提交的任务 集群和所有Job的详细状态都可以通过这个组件获取 ResourceManager 保存有所有正在运行中的JobManager、TaskManager的信息，通过心跳来监听它们的状态 TaskManager在创建时会上报Slot的信息 负责给Job Manager分配Slot，TaskManager在槽使用完成之后回报Slot可用 与DisPatcher运行在同一进程 SlotManager ResourceManager关于Slot的操作都会交给SlotManager管理 将所有注册的Slot维护成一张视图来处理Slot的分配和待处理的请求 当资源不够时，会向ResourceManager请求新的资源。 JobMaster master的运行是通过此类运行的，一个job会有一个对应的实例 在启动时会将JobGrapth转换成对应的ExecutionGraph SchedulerNG负责任务的调度、部署、执行 SchedulerNG.ExecutionGrapth.CheckpointCoordinator协调算子和状态的分布式快照。通过发送消息给相关的task来触发快照。 监测TaskManager的状态 ClusterEntrypoint 集群启动的入口 HighAvailabilityServices 高可用服务的集合，这些服务的功能包括：高可用存储、注册表、leader选举、分布式计数器 每个高可用服务都会有一个响应的选举服务LeaderElectionService和提取服务LeaderRetrievalService ResourceManager/JobManager leader选举服务通过LeaderElectionService实现，在new ResourceManager/JobManager()时，也会创建一个相应的LeaderElectionService, 当Manager启动后会调用选举服务的start(LeaderContender contender)方法进行通知；leader变更服务通过LeaderRetrievalService实现，在taskmanager 收到一个新的请求requestSlot()时，会通过LeaderRetrievalService来获取jobmaster的地址； MiniCluster123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public void start() throws Exception { synchronized (lock) { checkState(!running, &quot;MiniCluster is already running&quot;); LOG.info(&quot;Starting Flink Mini Cluster&quot;); LOG.debug(&quot;Using configuration {}&quot;, miniClusterConfiguration); final Configuration configuration = miniClusterConfiguration.getConfiguration(); final boolean useSingleRpcService = miniClusterConfiguration.getRpcServiceSharing() == RpcServiceSharing.SHARED; try { initializeIOFormatClasses(configuration); LOG.info(&quot;Starting Metrics Registry&quot;); // 指标注册表 // TaskManager JobManager在启动时都会创建一个指标注册表 metricRegistry = createMetricRegistry(configuration); // bring up all the RPC services LOG.info(&quot;Starting RPC Service(s)&quot;); AkkaRpcServiceConfiguration akkaRpcServiceConfig = AkkaRpcServiceConfiguration.fromConfiguration(configuration); final RpcServiceFactory dispatcherResourceManagreComponentRpcServiceFactory; if (useSingleRpcService) { // we always need the 'commonRpcService' for auxiliary calls commonRpcService = createRpcService(akkaRpcServiceConfig, false, null); final CommonRpcServiceFactory commonRpcServiceFactory = new CommonRpcServiceFactory(commonRpcService); taskManagerRpcServiceFactory = commonRpcServiceFactory; dispatcherResourceManagreComponentRpcServiceFactory = commonRpcServiceFactory; } else { // we always need the 'commonRpcService' for auxiliary calls commonRpcService = createRpcService(akkaRpcServiceConfig, true, null); // start a new service per component, possibly with custom bind addresses final String jobManagerBindAddress = miniClusterConfiguration.getJobManagerBindAddress(); final String taskManagerBindAddress = miniClusterConfiguration.getTaskManagerBindAddress(); dispatcherResourceManagreComponentRpcServiceFactory = new DedicatedRpcServiceFactory(akkaRpcServiceConfig, jobManagerBindAddress); taskManagerRpcServiceFactory = new DedicatedRpcServiceFactory(akkaRpcServiceConfig, taskManagerBindAddress); } RpcService metricQueryServiceRpcService = MetricUtils.startMetricsRpcService( configuration, commonRpcService.getAddress()); metricRegistry.startQueryService(metricQueryServiceRpcService, null); ioExecutor = Executors.newFixedThreadPool( Hardware.getNumberCPUCores(), new ExecutorThreadFactory(&quot;mini-cluster-io&quot;)); haServices = createHighAvailabilityServices(configuration, ioExecutor); blobServer = new BlobServer(configuration, haServices.createBlobStore()); blobServer.start(); heartbeatServices = HeartbeatServices.fromConfiguration(configuration); blobCacheService = new BlobCacheService( configuration, haServices.createBlobStore(), new InetSocketAddress(InetAddress.getLocalHost(), blobServer.getPort()) ); startTaskManagers(); MetricQueryServiceRetriever metricQueryServiceRetriever = new RpcMetricQueryServiceRetriever(metricRegistry.getMetricQueryServiceRpcService()); dispatcherResourceManagerComponents.addAll(createDispatcherResourceManagerComponents( configuration, dispatcherResourceManagreComponentRpcServiceFactory, haServices, blobServer, heartbeatServices, metricRegistry, metricQueryServiceRetriever, new ShutDownFatalErrorHandler() )); resourceManagerLeaderRetriever = haServices.getResourceManagerLeaderRetriever(); dispatcherLeaderRetriever = haServices.getDispatcherLeaderRetriever(); webMonitorLeaderRetrievalService = haServices.getWebMonitorLeaderRetriever(); dispatcherGatewayRetriever = new RpcGatewayRetriever&lt;&gt;( commonRpcService, DispatcherGateway.class, DispatcherId::fromUuid, 20, Time.milliseconds(20L)); resourceManagerGatewayRetriever = new RpcGatewayRetriever&lt;&gt;( commonRpcService, ResourceManagerGateway.class, ResourceManagerId::fromUuid, 20, Time.milliseconds(20L)); webMonitorLeaderRetriever = new LeaderRetriever(); resourceManagerLeaderRetriever.start(resourceManagerGatewayRetriever); dispatcherLeaderRetriever.start(dispatcherGatewayRetriever); webMonitorLeaderRetrievalService.start(webMonitorLeaderRetriever); } catch (Exception e) { // cleanup everything try { close(); } catch (Exception ee) { e.addSuppressed(ee); } throw e; } // create a new termination future terminationFuture = new CompletableFuture&lt;&gt;(); // now officially mark this as running running = true; LOG.info(&quot;Flink Mini Cluster started successfully&quot;); }} * ResourceManager leader election and leader retrieval * JobManager leader election and leader retrieval * Persistence for checkpoint metadata * Registering the latest completed checkpoint(s) * Persistence for the BLOB store * Registry that marks a job's status * Naming of RPC endpoints *","link":"/2019/12/04/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/"},{"title":"Flink源码分析-窗口","text":"窗口机制是Flink流处理的核心，它将无限元素的流分割成有限元素的集合（窗口）。当一个窗口不再增加新元素时，就可以对这个窗口中的所有元素执行计算逻辑。 前言从一个需求说起，业务方要实时计算每个商品在过去十分钟的展现、点击、购买数量来做运营活动，延迟不能超过一秒。商品的展现、点击、购买日志都会投递到消息队列，QPS大概在10W左右，计算结果存储在你redis中。 由于涉及到运营活动，数据的准确性和及时性非常重要。我们先思考下不借助框架，应该如何去实现这个需求，都有哪些实现方式？ 这种需求的实现方式有多种，其中一种就是Window(窗口)。大致过程为把无限元素的流按时间 分割成有限元素的集合（窗口）。当窗口中元素不再新增元素时，对窗口中的元素执行计算逻辑，然后将计算结果发送给下游。当窗口中如果元素数量过多导致占用过多内存时，就需要预先执行增量计算来减少元素数量。从这个过程中，可以总结出三个核心的功能点： 分割窗口方式 窗口达到稳定状态时机 计算方式 在实际的应用场景中，还要考虑边界条件： 窗口元素过多，导致计算太慢或者占用内存过多 当一个窗口结束时，又收到应该属于这个窗口的元素 窗口分割方式窗口类型： 滚动窗口 滑动窗口 会话窗口 时间： 事件发生时间 收到事件时间 处理事件时间 滚动窗口滑动窗口会话窗口窗口触发器窗口方法ReduceFunctionAggregateFunctionFoldFunctionProcessWindowFunction延迟 参考： Flink Windows","link":"/2019/11/26/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E7%AA%97%E5%8F%A3/"},{"title":"Flink源码分析- 高可用","text":"心跳ResourceManager，TaskManager，JobManager会通过心跳来监测相互之间的状态 高可用服务Leader选举和提取。只有配置了Zookeeper才是真正的高可用 重启策略当任务因为异常退出执行时，此次任务状态就会切换为失败状态。当TaskManager进程因为异常退出时，JobManager可以通过心跳服务监测到来重启服务。 重启策略有： 不重启 重启n次，每次延迟一定时间 状态恢复StreamTask在invoke()方法被调用时，会执行initializeState()来初始化算子中的状态，如果UserFunction实现了restoreState()方法，也会在这个过程中被调用来恢复之前的状态。","link":"/2019/12/03/Flink%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"title":"TCP","text":"介绍连接的生命周期 报文格式 序列号 包的序列号，用于控制包的按序传送 Ack server的回应，表示收到，用于可靠性传输 tcp flags tcp 状态机 window 流量控制 通过三次握手建立连接，连接有半连接队列和全连接队列。如果发生dos攻击，会出现大量的SYN_RCVD状态，可通过服务端主动发送一个特殊的ack来进行探测对方是否是恶意客户端。连接队列如果满了的话，会出现connection reset by peer异常。当server收到client的ack时，会将连接放到全连接队列中等待accept，这个时候如果server没有accept connection并且client在持续发送数据，就会造成客户端重传数据卡住。 client发送数据后，server可能会延迟回复ack 一般是配置为40ms 关键词 rtt 网络往返时间（round trip time） mss 最大分段大小 (maxium segment size)。tcp最大报文大小，不包含tcp header和option mtu 网络最大传输单元（maximum transmit unit）。多用于以太网口，如果要传输的数据包的大小超过mtu，有可能会丢弃包。 三次握手 握手过程syn-&gt; syn ack -&gt; ack 半连接队列 baklog 全连接队列 在连接队列满了的时候会出现 connection reset by peer 常见问题队列溢出 Connection reset by peer 队列溢出后的重传四次挥手挥手过程，中间状态 finish_wait_1close_waitfinish_wait_2last_ackTIME_WAIT TIME_WAIT大量出现的话会造成不能创建连接 常见问题 出现大量TIME_WAIT 为何四次挥手 fin -&gt; fin ack -&gt; fin -&gt; ack 传输过程 mss mtu maximum transmission unit delay ack nagle 启动慢启动，逐渐增大流量 可靠ack确认，重传ack 实际确认直接，deplay ack。 流量控制窗口，拥塞 常见问题 粘包 丢包 加速 队头阻塞（head-of-line blocking） dos攻击 调试工具 参考 最经典的TCP性能问题 TCP流量控制、拥塞控制 TCP之“滑动窗口”协议 TCP 的那些事儿 MAC地址表、ARP缓存表以及路由表 ip地址 mac地址","link":"/2019/12/16/TCP/"},{"title":"OLAP系统","text":"前言概述OLAP 联机分析处理（OnLine Analytical Processing）。OLAP系统是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 OLTP 联机事务处理（Online Transaction Processing）。OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。 发展背景自20世纪80年代开始，许多企业利用关系型数据库来存储和管理业务数据，并建立相应的应用系统来支持日常的业务运作。这种应用以支持业务处理为主要目的，被称为联机事务处理(Online Transaction Processing，OLTP)应用，它所存储的数据被称为操作数据或者业务数据。 随着数据库技术的广泛应用，企业信息系统产生了大量的业务数据，如何从这些海量的业务数据中提取出对企业决策分析有用的信息，这成为企业决策管理人员所面临的重要难题。因此，人们逐渐尝试对OLTP数据库中的数据进行再加工，以形成一个综合的、面服务对象、访问方式、事务管理乃至物理存储等方面都有不同的特点和要求，因此，直接在操作型数据库上建立决策支持系统是不合适的。数据仓库技术就是在这样的背景下发展起来的。 随着市场竞争的日趋激烈，企业更加强调决策的及时性和准确性，这使得以支持决策管理分析为主要目的的应用迅速崛起，这类应用被称为联机分析处理，它所存储的数据被称为信息数据。 联机分析处理的概念最早由关系数据库之父E．F．Codd于1993年提出。Codd认为，联机事务处理已不能满足终端用户对数据库查询分析的要求，SQL对大容量数据库的简单查询也不能满足用户分析的需求。用户的决策分析需要对关系数据库进行大量的计算才能得到结果，而查询的结果并不能满足决策者提出的需求。因此，Codd提出了多维数据库和多维分析的概念，即OLAP。OLAP委员会对联机分析处理的定义为：使分析人员、管理人员或执行人员能够从多种角度对从原始数据中转化出来的、能够真正为用户所理解的、并真实反映企业维特性的信息进行快速、一致、交互的存取，从而获得对数据更深入了解的一类软件技术。 OLTP和OLAP 数据处理类型 OLTP OLAP 基本定义 联机事物系统, 管理系统数据的修改 联机数据提取和数据分析系统 数据来源 OLTP系统操作数据 多个OLTP系统的历史数据 事物 频繁并且耗时短 不频繁并且耗时较长 面向对象 业务开发人员 分析决策人员 功能实现 日常事务处理 面向分析决策 数据模型 关系模型 多维模型 数据量 几条或几十条记录 千万上亿条记录 操作类型 查询、插入、更新、删除 查询为主 OLTP和OALP的关系 应用场景 产品报表，展示业务的经营情况 Adhoc查询 技术架构基本概念我们已经知道OLAP的操作是以查询——也就是数据库的SELECT操作为主，但是查询可以很复杂，比如基于关系数据库的查询可以多表关联，可以使用COUNT、SUM、AVG等聚合函数。OLAP正是基于多维模型定义了一些常见的面向分析的操作类型是这些操作显得更加直观。 OLAP的多维分析操作包括：钻取（Drill-down）、上卷（Roll-up）、切片（Slice）、切块（Dice）以及旋转（Pivot）。 Cube数据立方体只是多维模型的一个形象的说法。立方体其本身只有三维，但多维模型不仅限于三维模型，可以组合更多的维度，但一方面是出于更方便地解释和描述，同时也是给思维成像和想象的空间；另一方面是为了与传统关系型数据库的二维表区别开来，于是就有了数据立方体的叫法。 纬度人们观察数据的特定角度，是考虑问题时的一类属性，属性集合构成一个维（时间维、地理维等）。 指标多维数组的取值, 如浙江区域第一季度的完单量。 上卷钻取的逆操作，即从细粒度数据向高层的聚合，如将江苏省、上海市和浙江省的销售数据进行汇总来查看江浙沪地区的销售数据，如上图。 下钻在维的不同层次间的变化，从上层降到下一层，或者说是将汇总数据拆分到更细节的数据，比如通过对2010年第二季度的总销售数据进行钻取来查看2010年第二季度4、5、6每个月的消费数据，如上图；当然也可以钻取浙江省来查看杭州市、宁波市、温州市……这些城市的销售数据。 切片选择维中特定的值进行分析，比如只选择电子产品的销售数据，或者2010年第二季度的数据。 切块选择维中特定区间的数据或者某批特定值进行分析，比如选择2010年第一季度到2010年第二季度的销售数据，或者是电子产品和日用品的销售数据。 旋转即维的位置的互换，就像是二维表的行列转换，如图中通过旋转实现产品维和地域维的互换。 常用技术方案开场先来一张图 OLAP系统的流程： 前端将数据查询的结果以可视化方式展现出来，借助图形化的手段，让我们更加容易的去探索数据，发现数据中有价值的信息 分析师或者业务人员通过拖拽指标维度确定组合，组成自己想看的结果，结果查询出来后可以用各种合适的数据可视化图表进行展示 SQL Parser将用户对数据的描述转化为SQL并进行查询 查询引擎负责选择合适的数据库来执行SQL并返回查询结果 OLTP系统的数据和原始的日志信息，经过清洗处理最后统一存储。 影响分析师或者业务人员体验的主要有以下两点。 数据可视化图表是否丰富不同的图表展示的信息侧重点不一样，优秀的视觉效果和丰富的图表能让用户充分的表达出想要展示的信息。 响应时间是否满足预期在不同的场景下对响应时间的要求是不一样的，例如业务报表是对业务一周或者一段时间业务数据的展示，被展示对象希望的是打开系统就能看到报表，这种情况下响应速度可能就需要在毫秒级；如果在报表在分析有个业务数据出现了异常，就需要查询大量的数据定位问题，这种情况下响应时间越快就越能帮助分析师及时的定位问题；上线一个新业务，数据分析师需要查询大量的数据来验证业务的效果，这种情况下数据分析师对响应时间的要求可能就没那么高。 OLAP引擎选型OLAP需求大体分为两类: 即席查询：指用户通过手写SQL来完成一些临时的数据分析需求。这类需求的SQL形式多变、逻辑复杂，对响应时间没有严格的要求。 固化查询：指对一些固化下来的取数、看数的需求，通过数据产品的形式提供给用户，从而提高数据分析和运营的效率。这类需求的SQL有固定的模式，对响应时间有比较高的要求。 OLAP引擎的主要分类： ROLAP (Relational OLAP) 基于RDBMS技术，通过并⾏化/内存加速计算 代表：Presto / SparkSQL / Clickhouse 优势 ⽀持任意的SQL表达 ⽆数据冗余和预处理 不⾜ ⼤数据量、复杂查询下分钟级响应 不⽀持实时数据 适⽤场景 对灵活性⾮常⾼的即席查询场景 MOLAP (Multi-dimensional OLAP) 预先聚合明细数据，系统中只存储汇总数据 代表：Kylin / Druid 优势 ⽀持超⼤原始数据集 ⾼性能、⾼并发 不⾜ 不⽀持明细数据查询 需要预先定义维度、指标 适⽤场景 对性能要求⾮常⾼的OLAP场景 Search Engines 基于搜索引擎技术，通过索引加速计算 代表：Elasticsearch / Solr 优势 强⼤的明细检索功能 同时⽀持实时与离线数据 不⾜ ⼤数据量、复杂查询下分钟级响应 不⽀持Join、⼦查询等 适⽤场景 中⼩数据规模的简单OLAP分析的场景 目前还没有一个OLAP引擎能够满足各种场景的查询需求。其本质原因是，没有一个系统能同时在数据量、性能、和灵活性三个方面做到完美，每个系统在设计时都需要在这三者间做出取舍。在选择OLAP引擎的时候，也需要根据下图来分析当前的业务场景，以选择最合适的引擎。 以下是几种常用的OLAP引擎对比 网约车分析门户项目背景我们这次重构网约车分析门户的目的是优化分析门户的体验，配合数据分析师开发新的图表。 整体架构之前的网约车分析门户的技术架构如下图： 之前的架构主要有以下问题： Tableau图表不能自定义，不能完全满足数据分析师的需求 通过Hive查询速度过慢，在浏览器打开页面等待时间过长，甚至超时导致完全打不开页面 我们这次的重构，主要是以解决这两个问题为主，当前系统的架构： Superset是Airbnb贡献给Apache的一个开源项目。当分析师有新增图表或者UI需求的时候，都可以基于superset进行定制化的开发。Clickhouse是Yandex开源的一个数据分析的数据库，性能非常强悍，但是数据需要从hive表中导入。Presto是一个基于hive的查询引擎，性能大约是hive的10倍，可以直接从hive表中读取数据。Kylin是一个MOLAP类型的数据库，目前在项目中还没使用，之后会接入。 总结本篇文章对OLAP系统主要从以下几点做了一个简单的概述: 发展背景 应用场景 常见的OLAP系统架构 OLAP分析引擎类型及如何选型 之后会对常用的OLAP引擎架构做一个介绍和对比。 参考资料 Difference Between OLTP and OLAP (with Comparison Chart) - Tech Differences OLTP vs OLAP: what’s the difference between them? Shape Shape Shape Top 10 Best Analytical Processing (OLAP) Tools: Business Intelligence OLTP vs OLAP: What’s the Difference? What is OLAP (online analytical processing)? - Definition from WhatIs.com What is OLAP (Online Analytical Processing): Cube, Operations &amp; Types Not Acceptable! Data Warehousing OLAP Understanding OLAP on Big Data: Why do you need it OLAP on Big Data and its Business Impact 海量实时OLAP分析技术升级之路 主流OLAP系统对比总结 小米大数据：借助Apache Kylin打造高效、易用的一站式OLAP解决方案 一文读懂多维分析技术（OLAP）的进化过程-控件新闻-慧都网 开源OLAP引擎测评报告(SparkSql、Presto、Impala、HAWQ、ClickHouse、GreenPlum) - clickhouseclub 比Hive快800倍！大数据实时分析领域黑马ClickHouse入门 - 安全内参 | 网络安全首席知识官 【案例分享】Apache Kylin在美团点评的应用 大数据新秀，比Hadoop还好用的—-Presto！ presto、druid、sparkSQL、kylin的对比分析 怎样进行大数据的入门级学习？ 「案例」Kylin 在携程的实践（上） 分布式Ad-hoc查询系统 - scott_zgeng的专栏 - CSDN博客 OLAP为何被称作最强的商业智能工具？-控件新闻-慧都网 数据立方体与OLAP | 网站数据分析 Meetup-Druid和Kylin在美团点评的选型与实践.pdf OLAP_cube Data_drilling","link":"/2019/10/18/OLAP%E7%B3%BB%E7%BB%9F/"},{"title":"Jvm调优相关","text":"指标健康GC状况 YoungGC频率5秒/次 CMS GC频率不超过1天/次 每次YoungGC的时间不超过30ms(有文章:50ms) FullGC频率尽可能完全杜绝 Minor GC执行不频繁，约10秒一次 Full GC执行时间不到1s 这些指标仅供参考，实际情况应该根据业务情况来进行参考。 GC策略评价指标 吞吐量 应用系统的生命周期内，应用程序所花费的时间和系统总运行时间的比值。系统总运行时间=应用程序耗时+GC耗时。如果系统运行了100分钟，GC耗时1分钟，则系统吞吐量=99% 垃圾回收器负载 垃圾回收器负载=GC耗时/系统总运行时间 停顿时间 垃圾回收器运行时，应用程序的暂停时间 垃圾回收频率 垃圾回收器多长时间运行一次。一般而言，频率越低越好，通常增大堆空间可以有效降低垃圾回收发生的频率，但是会增加回收时产生的停顿时间。 反应时间 当一个对象成为垃圾后，多长时间内，它所占用的内存空间会被释放掉 方法查看程序执行时间 ps -p -o etime 查看程序的GC情况 jstat -gccause 2s 0 这个命令可以观察到以下几个指标: GC执行总时间 young gc执行次数，总时间，频率(time/count) full gc事件次数，不同的垃圾回收器指标代表的含义不一样 可通过观察这些指标来决定是否需要调整gc参数 查看Top 20对象 jmap -histo | head -n20 这个命令执行多次来观察哪个对象的数量或者内存一直在增长 打印线程堆栈 jstack dump内存 jmap -dump:format=b,file=app.bin 工具 5秒钟打印一次堆栈和top对象 1234567891011121314#!/bin/bashpid=$1mkdir $pidcd $pidwhile [ 1 -gt 0 ]do jstack $pid &gt; jstack-`date '+%H-%M-%S'` jmap -histo $pid | head -n20 &gt; jmap-top-`date '+%H-%M-%S'` sleep 5done 分析堆栈和top对象的变化趋势 分析工具MAT分析java堆 MAT从入门到精通（一） MAT从入门到精通（二） MAT 常用功能 jprofiler","link":"/2018/12/16/Java-jvm%E8%B0%83%E4%BC%98%E7%9B%B8%E5%85%B3/"},{"title":"申请美国区APP Store账号","text":"生活不易，翻墙不易啊 想来大家也知道，为了‘和谐’的社会，我们的局域网很多软件下载不了，很多网站也看不成。 由于我是做开发的，经常需要查一些资料，而这些技术资料，百度到的基本全是一模一样的，质量非常低。后来偶然发现google英文的资料，从量和质都高很多，但是局域网内不能访问google，这就需要翻墙了。 电脑端翻墙的资料已经很多很多了，手机Android端由于其开放性也很容易下载到相关的软件。 但iphone由于只能从app store下载软件，很容易就把一些软件给墙掉，所以就需要申请一个美国区的App Store账号。 我从网上搜索了一些资料，都是需要配合手机的App Store来激活美国区Apple Id的。其中有一个应该是最容易的方法，就是把手机连接上VPN，IP是美国的，这样就可以直接申请美国账号了，但是国内的账号下载不了VPN，下载不了就申请不成美国账号，申请不成美国账号就下载不了VPN，下载不了VPN就申请不成美国账号，妥妥的死循环。 知乎参考资料: 链接 上面废话一堆，下面是我的申请步骤: 一台mac，shadowsocks，美国的代理账号(mac安装shadowsocks非常容易了，搜下资料一堆) 打开shadowsocks， 使用全局代理 在网页上创建一个apple id账号，一定要把网页右下角的地区设置成美国 将付款方式选择none 按照提示填写信息即可，街道，电话信，可以在goole地图上自行找一个地址，也可以使用图片上的这个地址. 打开电脑的icloud设置，登录刚刚注册好的账号。 最后一步也是最终的一步，打开电脑的app store，随便安装一个免费软件。会弹窗，让你Review，根据提示来就可以，付款项选择None，如果不用全局VPN，这里可能就不会出现None这个选项。 成功安装一个软件之后，这个账号也就表示顺利激活了，在手机上就可以也可以使用。 还是没成功的话，可以发邮件","link":"/2018/08/09/america_app_store/"},{"title":"ZooKeeper介绍","text":"大部分高可用的场景中都会使用到ZooKeeper，例如：Hdfs，Hbase，Flink。 概要应用场景ZooKeeper应用场景非常广泛 分布式锁 高可用 Hbase的Master选举，Flink的JobMaster选举。。 发布/订阅 微服务注册中心 分布式队列 重要概念ZooKeeper对应用层来说，核心的就两点Znode和Watcher Znodezk是将所有的数据组织成一颗树，类似于文件的组织形式。例如:/flink/cluster, /flink 和 /flink/cluster都是Znode，其中后者是前者的子节点。 节点类型节点类型可以分为两个维度： 持久性 有序性 持久节点 有序节点 临时节点 普通 这两个维度一共可以组合出四种类型的节点。 永久节点 当节点被创建时，除非显示调用delete命令删除，否则节点会一直存在。临时节点 客户端连接与zk服务端连接断开时，这个节点就会被删除。有序节点 可以在一个节点下创建有序的子节点，每次创建都会生成一个带有序号的子节点。例如:create -s /lock_test/lock 1这个命令执行两次，会生成两个节点 /lock_test/lock0000000000 /lock_test/lock0000000001 节点包含很多属性，通过get命令可以看到Znode包含的属性。 12345678910111213[zkshell: 12] get /zk_testmy_data 数据cZxid = 5 创建的事务idctime = Fri Jun 05 13:57:06 PDT 2009 创建时间mZxid = 5 修改的事务idmtime = Fri Jun 05 13:57:06 PDT 2009pZxid = 5 修改children的事务id cversion = 0 子节点数据修改次数dataVersion = 0 数据版本 数据修 改次数aclVersion = 0ephemeralOwner = 0 如果是临时节点，表示session id。否则为0。dataLength = 7numChildren = 0 children个数 节点分为临时节点和持久节点，临时节点在session失效时就会被删除，持久节点会一直保存。 Watcher当Znode有变动时，会通知给监听者。Master选举的过程就是master节点会在zk中创建一个znode，拿到这个master节点权限的节点就会做为主master节点，当主master节点异常挂掉时，备用节点会收到通知，然后拿到znode权限的节点就会升级为主master了。 集群架构 集群中会有多个节点，一个Leader节点 选举过程 zab协议 群首执行Client提交的请求，并形成状态的更新，就称之为事务。事务是原子性执行的，Znode数据和版本变更是原子的。事务还具有幂等性。 保障： 保证同一时刻只有一个leader 事务严格按照顺序来执行 服务器中还是可能存在两个leader的，但是由于一个提案需要半数机器同意才可以，而一个新的leader产生也需要半数以上节点同意才可以，这样就会保证支持新群首的仲裁机器与确认事务T的仲裁机器至少有一台是重合的，这就能保证了系统的正确性，事务不会丢失而且系统状态也不会被破坏。 存储zk中的数据都是存储在内存中。 持久化 快照事务日志 故障处理考虑以下的情况： 客户端提交事务到zk，zk在事务成功提交之后，client收到相应之前，网络断掉 客户端在发送请求之后，zk leader 宕机 zk 在响应一个事务的过程中，leader宕机 多集群中的一个节点与zk建立连接后，身份变更开始更新自己系统的数据。因为系统因为负载过高与zk断开连接，但是其已经将更新操作放入执行队列中，等待cpu时钟周期。 一个分布式系统使用zk来选举master节点，当一个节点与zk断开连接时，其它节点接管master，这个时候会出现双master情况吗? https://zhuanlan.zhihu.com/p/72176940 taobao zk zk 应用场景 Zookeeper: 分布式过程协同技术详解","link":"/2019/12/10/ZooKeeper%E6%A6%82%E8%A6%81/"},{"title":"Flink-异步operator","text":"当某个operator执行需要很长时间的话，使用异步操作对吞吐量的提升非常有帮助。 其实过程重要基于以下几个步骤： 处理element过程异步化，将处理element的过程提交到一个线程池中 处理结果完成后，将结果提交给ResultFuture中 ResultFuture提交到结果队列中，队列分为有序和无序两种 有序队列中，有一个ResultFuture队列，如果有任何一个ResultFuture完成，会调用headIsCompleted.signalAll()来通知正在等待结果的线程有新元素到来 无序队列中，有两个集合，completedQueue,firstSet 当有新元素近来时会先放置到firstSet中，当有处理过程完成时，将结果放到completedQueue中，并且调用hasCompletedEntries.signalAll() Emitter会开启一个专门的线程，这个线程中会调用queue.peekBlockingly()来监测是否有新的结果到来，当有结果时，调用output.collect将结果传递给下游 AsyncWaitOperator继承自AbstractUdfStreamOperator，在初始的时候会开启Emitter线程 由于DataStream没有直接支持AsyncWaitOperator，因此调用DataStream.tranform来将AsyncWaitOperator添加到流中 UserFunction继承RichAsyncFunction，初始化时开启一个线程池用于异步处理element。重写asyncInvoke(input, ResultFuture)方法，将处理过程封装成Runnable提交到线程中，完成之后调用ResultFuture.complete来执行后续的流程","link":"/2019/12/23/flink-RichAsyncFunction/"},{"title":"随手记","text":"少打游戏，少看视频，多读书。这就是现在想的，可是没做到啊。 ORZ，还是高中最后一年大锅的那个教室好啊，除了学习就是考试，除了考试就是看排名升了还是降了，反正一个字，就是干！等到大学考研的时候才真切体会到有个好老师是多么幸福的事情。因为考研自己给自己操碎了心，青发随风飘向他乡了不少。 现在就是随便写点东西，告诉这个人：你不要再那么懒，多看书，多思考，多记录，少做梦! 争取将来能够翻墙喝奶粉，打疫苗!!!","link":"/2018/07/22/suishouji/"},{"title":"技术驱动","text":"最近在做一个数据分析门户的过程中，引发了一个思考：我应该怎么才能把这个项目做好? 不仅仅追求技术上的精进，也能够跳出来看业务全局。 不仅能解决技术难题，还会主动关注业务，用技术加速业务的成功。 不要将业务和技术割裂来看，它们应该是一体的。 业务的需求能够促使技术进行创新，业务上的成功才能让技术产生意义。 技术也能为业务提出更好的解决方案，甚至提出新的业务思路。由于业务人员不一定是技术出身，不一定能将技术的价值发挥到最大，如果技术能主动去理解，对同一个需求，从技术的角度可能会提出优化的方案。例如OLAP系统中运营、BI可能已经使用习惯了当前的查询速度，他们不知道还能再通过技术手段来给查询提速，这个时候，作为技术人员应该去调研当前的技术方案，用户的需求，分析速度的提升带来的收益是什么，业务效率提升，节省人力。 对当前技术不能实现的需求，也不要直接拒绝，应该深入的去理解这个需求目的是什么，为什么要提这个需求，如果当前的技术不能满足的话，能不能换另外一种折衷的技术方案来去满足业务需求。 业务需要技术支撑，技术需要业务反哺","link":"/2019/12/15/%E4%B8%9A%E5%8A%A1%E4%B8%8E%E6%8A%80%E6%9C%AF/"},{"title":"生活","text":"那年背包懵懂入校，那是09年，转眼已过九年，那年不到20，现在将近30。 工作已有两年，算上实习将近4年，时间也不算短了。茫然的随波到大三时，对未来仍然没有概念，就又随波的考了个研。然后就找实习，找工作。陡然的到了当前，要成家，要立业，要养家，要糊口。考虑的事情多了，对未来是个什么东西就有了一点当下的思考。 30岁时，父母，孩儿，妻，需要保证父母身体健康，孩儿奶粉，家庭和睦。这是30岁时的设想。而自己是正当壮年。 35岁时，父母，孩儿，妻，需要保证父母不再劳作，保养身体，孩儿上学，家庭合睦。这是35岁时的设想。后浪已成长起来，作为前浪已有点危险。 再往35岁之后就不看了，那是30岁之后需要看的了。看，这就是未来，它很清楚的摆在你面前。未来是什么呢？我觉得主要是根据当前环境和可预期的变化再加上自己的一点期望来大致勾画下后几年的场景。有了场景，再反过来看下当前，做个深刻的反思，依照当前的环境，当前的作为，在30岁是会什么样的，在35岁时又会是什么样的？后浪在汹涌拍过来的时候，你是怕还是不怕？ 反正，在现在有了这个未来这个压力之后，我是怕的。 我现在就在想，我需要做些什么，改变些什么，才能让未来的我可以勾画出未来那副设想好的场景，或者给这场景更是能够添上几朵好看点的花。 以次篇短文来纪述下一点思考。","link":"/2018/06/28/xiaoji/"},{"title":"阶段提交","text":"ACID原则ACID是一种描述一致性的原则，通常出现在数据库系统中。 A Atomicity: 原子性，事务中的所有操作要么不成功，要么都成功 C Consistency: 一致性，外界访问数据库中的数据时，不会访问到事物的中间状态，只会访问到最终的状态 I Isolation: 隔离性，事务可以并发执行，不会互相影响。 D Duration: 持久性，一旦事务提交成功，状态的改变是持久的，不会失效。 单机事务一般是需要满足ACID的。 分布式事务需要多节点协作来完成一个事务，其实现方式和单机事务有很大不同，也很难满足ACID原则，实现方式和单机事务的实现方式也有很大不同。目前分布式事务是通过阶段提交来实现的，阶段提交分为二阶段和三阶段提交。 二阶段提交可将其分解为预提交和正式提交两个阶段。 预提交：协调者（Coordinator）发起提交某个事务的申请，各参与执行者（Participant）需要尝试提交并反馈是否能完成。 正式提交：协调者如果得到所有执行者的成功答复，则发出正式提交请求。如果成功完成，则算法执行成功。 在提交过程如果出现问题，事务就需要回退。 优点是实现简单，但缺点很明显就是它的整个过程是阻塞的，需要等到所有节点完成才能执行下一步，会使性能较差。当一个节点出现问题，可能会造成事务一直无法提交。在第二个阶段协调者和参与者出现故障，也会造成数据不一致的情况。 三阶段提交三阶段提交其实就是对二阶段提交的扩展，分为三步 尝试提交 协调者会询问各个节点是否能参与投票，如果不能就不会再执行下去 预提交 如果所有节点都返回可以执行投票，所有的参与者就会尝试进行提交并且反馈是否能够完成。 正式提交 协调者如果得到所有执行者的成功答复，则发出正式提交申请，如果成功完成，则本次事务成功 三阶段提交只是在一定程度上缓解了提交冲突问题，还是无法保证数据一致性的问题。一致性可以参考Paxos","link":"/2019/12/11/%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/"},{"title":"分析门户","text":"序言数据工程师（ DE ）和数据产品经理（DPM）日常的一部分工作是和运营、PM对接，根据需求产出APP层数据和可视化报表。 DE分别通过Tableau和数易来产出报表，但这两个产品都有局限性： Tableau价格昂贵，使用人数有限制 数易的可视化图表类型过少，不能满足需求 且DE有个性化的需求，Tableau和数易均不能给出及时的响应 然后，数据分析门户就应运而生了。 产品介绍用户从序言的背景中了解到，使用分析门户的用户主要DE、DPM、PM、运营。由于报表中会包含各个业务线的重要指标，所以各个业务线的负责人也会通过分析门户来查看他们关注的指标。 从用户职责的角度来划分用户的话，可以将用户划分为以下几层： DE负责产出APP层数据，诉求为： 产出的数据能方便的接入到分析系统中 DPM负责制作报表，主要有以下两个诉求： 丰富的图表类型，并且能支持图表的个性化开发 各种粒度的权限控制（表权限、行级权限） PM、运营、业务线负责人会经常查看各种关键指标，他们的主要诉求如下： 图表类型丰富、配色适当，能够方便快速的理解指标代表的含义（柱状图、折线图、热力图） 响应速度快，通过网页打开一个报表，如果速度太慢，会严重降低用户体验 这几层用户和我们系统相互之间的关系，可以用下图来表示。 从上图可以看出当前制作报表的简要流程为： 报表需求方提需求给DPM DPM制作好报表给到需求方 如果报表需求方对报表有细微的改动，或者需要做一些简单的分析，还需要再重复步骤1、2。这个过程不仅周期较长，而且还会耗费人力。 从上面的流程可以看出，DE、DPM负责数据和报表的产出，其它用户不做内容产出，仅仅浏览DPM产出的报表，和我们的系统交互较少，因此DE、DPM为我们当前系统的核心用户。如果我们能够解决步骤三的问题，降低使用门槛，让需求方自己也能非常方便的做一些简单的分析，而且能够产出报表。不仅能增加用户对系统的使用频次，也能扩大我们系统的用户群体。 功能介绍分析门户当前是基于开源系统Superset，它中文翻译是快船，是一个自助式数据分析工具，它的主要目标是简化我们的数据探索分析操作。 当前分析门户主要分为两个部分： 前端门户：统一展示看板列表 后台配置系统：制作看板 当前门户支持的功能有： 用户可自主在权限系统上申请对应看板权限 集成sso登录 以可视化方式配置看板 支持接入多种数据库，mysql, kylin, clickhouse, presto, druid等等 superset系统的详细实用方式可以参考 文章 工程介绍后端是基于Python的，用到的框架有: Gunicorn，在本地开发模式下不需要使用此框架 Flask AppBuilder(鉴权、CRUD、规则） Pandas（分析） SqlAlchemy（数据库ORM） 前端自然是JS的天下，用到的框架有: npm、react、webpack d3 (数据可视化) nvd3.org(可重用图表) echart 整个系统是典型的MVC架构，现在简要的介绍几个目录功能： assets 是前端代码目录 models 负责与数据库的交互，执行从各个数据源查数的逻辑 views 控制器，负责接收请求 templates 前端html模板 viz.py 负责前端各种图表数据格式的转换 __init__.py superset在启动时首先执行这个脚本，用于做一些初始化工作 custom 对引入的一些python package打一些补丁，修复bug；扩展viz.py图标类型；","link":"/2019/06/23/%E5%88%86%E6%9E%90%E9%97%A8%E6%88%B7%E7%B3%BB%E7%BB%9F/"},{"title":"分布式事务","text":"ACID和BASEACID是指原子性（Atomic）、一致性（Consistency）、隔离性（Isolation）、持久性（Duration）。 原子性 事务中的所有操作要么都成功要么都失败 一致性 外界访问数据库中的数据时，不会访问到事物的中间状态，只会访问到最终的状态 隔离性 事务之间不会相互影响，避免脏读、幻读给业务带来数据不一致问题 持久性 事务一旦执行成功，其最终状态会被持久化存储下来，不会失效。 BASE是指基本可用（Base Available）、柔性状态（Soft State）、最终一致性（Eventual Consistency）。 基本可用 是指分布式系统出现故障时，允许损失部分可用性，保障系统基本可用。 柔性状态 是指允许系统出现中间状态，而该中间系统并不会影响系统的整体可用性。 最终一致性 系统在经过一段时间之后，系统中的所有副本最终会达到一致的状态。 ACID和BASE代表了两种截然相反的设计哲学。ACID是传统数据库常用的设计理念，追求强一致性模型；BASE支持的是高效的分布式数据库，通过牺牲强一致性来获得高可用性。不同的业务场景对数据的一致性要求不一样，因此这两种设计理念也可能会结合在一起使用。 分布式系统内通信和单机内通信的最大区别是：单机不会丢信息，而分布式系统可能会丢失。一台机器通过网络向另外一台机器发送消息的结果可能是成功、失败、不知道成没成功，这也是分布式系统数据同步代价高昂的原因。 共识算法副本的一致性。Paxos、Zab、Raft解决的是副本在多节点的同步问题。读写流量走的都是leader。这会产生一些问题： 如果直接读取follower，可能读取的不是最新数据 如果为了最新的数据，读写流量都走leader，则leader的负载会远高于follower 如果出现了网络分区，出现两个leader，则从leader读取的数据也可能不是最新的 柔性事务如何解决分布式事务问题引入日志和补偿机制类似传统数据库，柔性事务的原子性也是由日志来保证，例如mysql的事务。事务日志记录事务的状态、参与者。参与者节点记录当前操作的对应的REDO或者UNDO日志。当事务重试、回滚时，可以根据这些日志最终将数据恢复到一致状态。 为了避免单点故障，事务日志是记录在分布式节点上的。REDO/UNDO日志一般记录在业务数据库（参与者节点）上的，可以保证节点操作与日志记录同时成功或者失败。当事务失败时，可以根据日志记录找回事务当前的执行状态，来决定是重试异常步骤（正向补偿），还是回滚（反向补偿）。 可靠消息传递在分布式环境下，由于“网络危险期”的存在，节点间的消息传递可能出现成功、失败、不知道成功还是失败三种状态。这给分布式事务的处理带来了非常大难题，解决这个问题就需要使节点之间的消息能可靠的进行传递。 当出现不知道成功还是失败的状态时，只有两种处理方法：1. 重复投递一次（至少一次）；2. 不重复投递 （至多一次）。在分布式事务处理过程中显然不能使用第二种处理方法，不然会丢失数据，就只能选择第一种方法。 至少一次的处理方法虽然不会丢失数据，但可能会使一条消息被重复处理。这也会造成问题，例如重复添加记录、重复扣款。这就需要处理程序必须实现幂等操作，对一个数据处理一次和多次结果一样。幂等的实现方式有很多种，需要结合业务来实现。如果是插入数据库，可以根据主键来实现幂等；如果是确定性计算并且数据是覆盖操作，则原生就是幂等的；消息里有唯一主键，消息处理程序根据唯一主键来进行去重。 实现无锁 不是特别明白这个处理方式和分布式事务的关系在哪里，这个处理方式只是为了高性能而已 造成数据库性能和吞吐的瓶颈主要是强事务（传统数据库的事务、阶段提交）带来的资源锁。因此避免事务的加锁是数据库高性能的关键，尤其是长时间事务。无锁是一个解决方法，但是事务隔离是仍然要考虑的一个问题。如果没有事务隔离，可能会出现数据的脏读、幻读，这在业务是不可接受的事情。 实现事务隔离的方法有多种，选择何种隔离方法应结合业务。 事务避免进行回滚 当事务异常时，如果不进行回滚也能满足业务的要求，也就是事务的中间状态即使被外界感知到也不会影响到业务的正常流程。由于事务不会回滚，因此不会出现脏读。 辅助业务变化明细表 乐观锁 TODO: 对无锁的实现方式和分布式事务之间的关系，以及分布式无锁化的实现方式需要进一步查询资料。 分布式事务的实现方法分布式事务的实现可以具体场景的维度来分析，分布式数据库的事务实现方法(tidb、zookeeper)和业务场景上（交易场景）的分布式事务可能就截然不同。 像tidb、zookeeper要求的是强一致性，中间状态不会被外部感知到，会损耗部分性能来达到数据的强一致性。而业务场景上，对性能要求很高，通常可以将一个大的事务分割成很多小的事务，中间状态可以被外部感知到，只要达到最终的一致性就可以。交易场景下，就可以分割成下单、扣减库存、支付等更细粒度的场景。所以在谈一个分布式事务的时候，应该具体问题具体分析。 强一致性分布式事务和柔性分布式事务的实现也不是完全对立的。一个大的分布式事务局部也可能会使用强一致性事务，而在全局上还是柔性分布式事务的实现方法。例如一个交易场景中，库存修改和日志的记录就是强一致性的。 柔性分布式事务的实现消息分布式事务 淘宝TXC事务框架 参考 企业IT架构转型之道_阿里巴巴中台战略思想与架构实战 Percolator 和 TiDB 事务算法 线性一致性和 Raft seata 蚂蚁分布式事务框架","link":"/2019/12/15/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"title":"参考资料","text":"方法 如何成为技术大牛 你的编程能力从什么时候开始突飞猛进- 阿莱克西斯的回答 架构设计实践五部曲 《软件架构设计》 企业架构 what is togaf 分布式通识 区块链技术指南，简单的描述了分布式系统的核心概念 一致性与共识 共识与一致性的区别 一致性分类 介绍一致性的分类，相关 文章1 raft动画 raft动画，形象的展示raft的整个过程 Raft一致性算法笔记 端到端一致性 存储 zookeeper选举过程 zookeeper简介 后端服务 JVM调优相关资料 一致性哈希 用于集群扩展 性能指标 Linux高性能服务器设计 Java HashMap synchronized分析 AQS实现 gc调优 频繁young gc 算法题 什么是动态规划 动态规划套路 高楼扔鸡蛋 算法题类型 AVL树，红黑树性能对比 漫画 B+树 KMP 算法面试必须掌握的14种模式 算法题分类 工具 亚马逊AWS云EC2搭建SS服务器 idea 激活服务器http://fls.jetbrains-agent.com","link":"/2019/12/11/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"},{"title":"学习的方法","text":"引用一个大神的两句话： 我想更深入的探讨一个系统的设计是被什么现实和本质问题所逼迫的结果从乱糟糟的观察到的混乱表象中，抽象出流系统所面对的问题和解决方案的本质文章连接 珍惜遇到的每一个难题 系统的好坏与否的一个本质问题：是否能尽可能的分析出系统所面临的边界条件。边界条件越多的系统，复杂性也越高。 在整个技术体系里，大致可以把常用的工具分为三类： 语言 语言 是否编译 性能 Java 半编译 中 Go 编译 中高 Python 解释 低 Php 解释 低 C++／C 编译 高 数据库维度：行列、分布式、性能高低、消息队列、缓存。常用组件有 mysql，hbase，clickhouse，hive，kylin，kafka，redis。 计算框架 维度：流处理、批处理。常用组件有 Flink，Storm，Spark，MapReduce。 在实际工作中，接触到的东西远不止上面列出的，还会有各种各样的框架和技术概念，例如：spring、spring cloud、djano、yii、tensorflow、微服务的各种框架。而技术框架又是在不停的推陈出新的，做为一个工程师，如果不持续的去学习新东西，可能很快就会被淘汰。这也造成了我们一直在疲于奔命的现象。 我的日常工作中，接触过很多种的工作，后台管理系统、后端服务、微服务、OLAP系统、流计算。每种方向用到的框架都不同，而且每种方向都有很多框架。满足工作需求很简单，看看demo，文档基本就能开始工作。 如果长久下来只是流于表面，对个人成长基本上是没有帮助的。 个人根本是没有任何收获的，非常容易被替代掉. 如果只是知道这个工具怎么用，一个新人花点时间也就能学会，老人相比于新人也就没什么大的价值了。 沉淀很少 如果用到的框架被另外一种替代了，或者更新了。该何去何从？是要重新学习新框架呢，还是要重新学习呢。 因为这么些原因，我时常有这些疑惑: 为什么要有这么多框架 两个组件功能相似，一个已经流行了，为什么另外一个还能流行起来甚至替代原先的框架 这些现象背后的根本原因是什么？ 在有了上面的疑惑之后，我又抽象出来了一些问题： 一个系统要解决的本质问题是什么？ 它是基于什么样的理论被创造出来的? 这类系统相关的边界问题是啥？ 两个系统是在哪个点上差生分叉的，不得不分叉的理由是什么 理论和实践的关系 我把解决这些问题看作解数学题的过程。我们平常做的数学题可以看做是一个个系统，而公式可以看做是这些系统背后的理论。 先找资料，找不到看源码。 技术深度是非常重要的，在最常用的技术上深度是非常重要的，例如操作系统、网络、JVM调优。但是当承担的任务大了的时候，广度就会变得更重要一些。 做技术选型时，就需要首先技术广度足够才可以，可以清楚的知道每种中间件的用途、主要应用场景，还有业务未来的发展方向，综合下来才可以选定当前业务的技术栈。 个人精力是有限的，而各种中间件、框架又是在快速的迭代发展，很难把每种中间件、框架都研究透彻的。面对这种情况，我认为一个人最核心的能力应该是如何将看起来复杂的问题进行抽象。在知道如何抽象之后，一个方针来去指导如何学习，以及怎么才能快速的摸清一个中间件是否满足需求，并且在遇到问题时如何快速定位。 构建知识体系，本质是对所学知识的抽象的一个过程。剥离所学知识的表象，找到其本质，追根朔源，了解理论／技术产生的北京，要解决的问题，各个知识点之间的关系，理解其脉络，才能灵活运用。 理论的变化是非常缓慢的，基于理论的实践而产生的事务是非常多变的。 类似的软件基于同一理论，又在某个功能上产生分叉。理论上不可行，但在工程实践中又宣称实现了这一理论必定做了某种折衷，理解一个框架就要理解清楚其背后的理论依据。从理论出发，可以摸透一个软件的脉络，它折衷的地方在哪，可以提出优化方向，洞见某个领域的发展，跟上时代的潮流，甚至走在领域的前沿。","link":"/2019/12/13/%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95/"},{"title":"微服务","text":"背景业务发展，功能太多，项目臃肿，迭代太慢 架构演进 http rpc服务间通信基本上是通过两种方式进行通信的http和rpc 应用场景 HTTP由于其通用性，接入代价小的特点主要用于外部服务调用 内部服务间的调用主要使用RPC 性能 RPC 和 HTTP的数据都是通过TCP进行传输 通用定义的http1.1协议的tcp报文包含信息比RPC使用的传输协议的tcp报文会多很多，所以其在传输效率上RPC一般来说会高一些 接入方式 HTTP由于其通用性，基本上所有的语言都可以非常方便的访问HTTP服务，服务方不需要了解业务方使用的何种语言。 RPC可能需要为业务方提供多个开发语言版本的SDK，使业务方可以像在本地一样调用服务，但服务方的开发量较大。 基本上各个语言都有性能良好的HTTP客户端库，而RPC的需要服务方针对业务方来提供经过测试的性能良好的SDK。 统一的rpc框架 服务标准化：上下游交互通过标准SDK形式划分，通过SDK提供统一的、一站式的服务发现、容错调度、监控采集等，进而降低服务开发、运维成本。 内置服务发现：对于未接入DiSF的模块来说，带来的收益是显而易见的：节点管理标准化、可运维化、快速扩容。 故障摘除：类似下游上线、机器故障、下游过载超时 导致的业务错误将不再存在，对提升业务自身的SLA有较大收益。 标准化日志输出&amp;采集：RPC通道日志将以标准化的形式打印并采集，统一集成到把脉、Metrics等，降低业务的运维负担。 微服务 特点：应用按业务拆分成服务各个服务均可独立部署服务可被多个应用共享服务之间可以通信 好处：架构上系统更加清晰核心模块稳定，以服务组件为单位进行升级，避免了频繁发布带来的风险开发管理方便单独团队维护、工作分明，职责清晰业务复用、代码复用非常容易拓展 挑战： 服务间依赖关系复杂 更加方便的负载均衡 动态扩容，缩容 故障摘除 服务降级 服务治理服务注册 服务自注册 第三方注册 服务发现 服务端发现 客户端发现 注册发现：动态扩缩容，上游零感知负载均衡：加权轮询策略自动探活：10s级健康探测，分钟级故障机器摘除依赖管理：管理自身消费了哪些下游服务 参考 浅谈服务治理与微服务 谈服务发现的背景、架构以及落地方案 SOA服务治理方案 微服务介绍 [浅谈服务治理与微服务]","link":"/2018/10/18/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"title":"数据一致性","text":"一致性(consistence)和共识性(consensus)不是完全一致的概念。 数据处理语义 Exactly Once 精确一次 At Least Once 最少一次 At Most Once 最多一次 在处理数据时候，需要考虑在三种不同的语义会对业务产生什么影响。程序要实现不同的语言代价也是不同的，而且不同的语义对业务的影响也不一样。 数据处理过程对数据一致性的影响是强相关的。 共识性共识性(consensus)和数据一致性(consistence)是两个很相似的概念，也经常被用混。共识指的是在一个分布式集群中，所有节点对同一个提案（proposal）能达成一致。提案生效时表示所有节点都达成一致，这个过程中也就不存在数据是否一致的问题了。 而一致性一般指的是分布式系统中一份数据的多个副本，对外表现的一致性。 一致性分类数据一致性是分布式数据存储的一个核心问题，它可以从两个维度来看： 以数据为中心 客户端在收到服务端对修改数据请求的响应后，此时数据在服务端中的备份处于哪种状态？ 以用户为中心 客户端在收到服务端对修改数据请求的响应后，立即再去读取相应的数据时，接收到的数据是什么样的？ 以数据为中心 严格一致性 任何写操作都能被读到，任何读操作读取的都是最新的数据。要达到这种一致性就需要有全局时钟，即每次操作都会有一个全局时间，以全局时间来看，我们的操作就是单进程的操作。 全局时钟基本实现不了。 顺序一致性 任何进程读取到的数据可以不是最新的，但是任何进程读取的数据的顺序必须和写操作的顺序是一致的。 因果一致性 如果一系列写入操作按照某种逻辑发生，那么读取也应该按照这种逻辑顺序读取。即如果 w(b) 依赖于 w(a)，那么用户读取的顺序也应该是 r(a) &gt; r(b)。如果 w(b) 与 w(a)之间没有任何依赖关系，即使在w(a)先于w(b)发生，也有可能先读取到r(a)。因果一致性需要借助于分布式逻辑时钟实现。 FIFO 一致性 同一个进程中的写操作，被看到的顺序必须是一致的。不同的进程的写操作被看到的顺序可以不一致，即使相互之间有因果关系。 上面的几种一致性其实是逐步弱化的，但是实现复杂度越来越简单，程序性能也是逐步增强的。 用户为中心 单调读一致性 顺序发起三次read请求，按读取到的数据的更新时间降序排列：r3 &gt;= r2 &gt;= r1，即最近一次读取的数据更新时间肯定不会晚于之前数据的更新事件。 单调写一致性 顺序发起三次write请求（w1, w2, w3），这三次请求的执行顺序必须要按照提交(w1 w2 w3)顺序来执行，即最近一次的写请求在执行之前必须要保正它前面的写请求都已经执行完毕。 如果不满足此一致性，可能会出现数据覆盖的问题。 写后读一致性 在一次write请求之后，发起read请求，读取的肯定是这次write请求的值。 如果程序不满足的话，如果刚发了条微博，要过一会才能读取到我刚刚发的这条微博。 读后写一致性 在发起一次read a请求之后，基于读取到的值a1发起了一次write a2，保证a2是基于a1的。 业务和一致性在谈一致性的时候，是必须要结合当前业务来看的。我们需要清楚的知道哪种一致性保障满足当前业务的需求。 业务上的一致性在业务开发时，经常谈到的一致性会经过简化。 强一致性 数据在写入后，在任意时刻都能从所有副本上读取到最新的值。 线性一致性 数据在写入成功之后，再次读取到的都是最新的数据。其表现形式在系统的用户看来是强一致性的。共识算法和基于共识算法的额外工作可以实现，tidb是这样实现的。 弱一致性 数据在写入后，有的副本可能已经更新，有的副本可能还没更新，不保证经过多长时间，所有的副本才会更新。 最终一致性 数据在写入之后，经过一个时间窗口最终所有副本都会达到一致。时间窗口是个非常重要指标。 我认为在谈论一致性的时候，应该结合业务来看，系统对一致性的要求越高就会导致系统实现的复杂度越高。而且在谈论一致性的时候，应该也加上业务的维度。 全局一致性 即我们在保证数据项a一致性的时候，应该对任何访问系统的用户都是一样的保证。 如果不满足此一致性，可能会导致不同的用户查看同一个数据项a的时候看到的结果是不一样的。 单一用户一致性 数据一致性保证只是针对单个用户来说，也就是将&lt;用户，数据项&gt;作为一个整体来保证一致性的。 场景在使用mysql主从的时候，不同的技术方案，可能会导致用户在访问我们的系统时得到不同的结果。这个时候需要结合业务来看，我们的系统应该保障什么样的数据一致性。 微博更新 博客系统 分布式系统一致性分类 分布式共识 被误用的一致性 一致性问题 Lamport Clock 分布式系统：Lamport 逻辑时钟 tidb和raft","link":"/2019/12/12/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E5%88%86%E7%B1%BB/"},{"title":"多进程网络服务","text":"背景近来在优化一个java项目的性能，在服务架构、gc、代码实现方式都做了基本的优化后，思考如何对其进行更进一步的优化。进一步的优化有两个方向： 使工程本身（架构、gc、代码）再进一步。 验证类似nginx一样的多进程网络服务是否可行。较容易实现，且很容易应用到其它线上服务上。 选择哪种方式，需要在业务要求、收益、成本之间做出平衡。 实现多进程网络服务是指一台服务器上监听相同端口号的多个进程组成的一组服务。 多进程监听相同端口号有两种方法： socket句柄重用 端口重用 socket句柄重用socket会话流程图 其中int sockfd = socket(domain, type, protocol) 会返回一个整数，然后通过fork()来创建子进程就可以达到多进程共用同一个socket句柄。这样就能实现多个进程监听同一个端口的效果。 这种方法会导致惊群问题。当Client调用connect() 与Server建立连接时，Server所有的进程都会被唤醒，但是只有一个进程能成功建立连接，其它进程然后继续休眠。accept()和epoll_wait()系统调用都会导致惊群问题，不过在linux2.6内核之后，accept()不会再导致惊群问题了。nginx中解决问题是通过在进程间加锁，进程间加锁有两种方法：共享内存和文件锁。 引申话题：进程间通信方式都有哪些？ jvm不支持fork，负作用很多，垃圾回收器和jit编译器的内存部分在fork之后会发生变动，共享内存也会被分离。所以如果java要想以这种方式来监听同一个端口非常麻烦。但是仍有方法来实现，大致过程如下： 开启一个进程创建socket 获取socket句柄的整数值fd 在这个进程中直接开启其它进程（不是通过fork方式） 将fd以参数传递给子进程 以fd重新构造ServerSocket 不过这个过程需要用到java的反射机制，在java1.9之后好像因为安全原因不能再使用这种反射方式了。相关的开源库 Multi-process network server。 端口复用 SO_REUSEPORTLinux内核(&gt;= 3.9)支持SO_REUSEPORT特性，不过之前的内核版本也有通过打补丁的方式支持这个特性。java是从java9之后支持的。 TCP/UDP连接是通过一个五元组来唯一标示的：{&lt;protocol&gt;, &lt;src addr&gt;, &lt;src port&gt;, &lt;dest addr&gt;, &lt;dest port&gt;}。默认情况下，操作系统中不允许重复绑定源地址和源端口。启动服务有时就会遇到address in use的异常，就是这个原因。 但是通过给setsockopt(fd, SOL_SOCKET, SO_REUSEPORT, &amp;reuse_port, sizeof(reuse_port)); 就可以复用端口，重复的绑定source_addr, source_port，只通过给socket设置一个参数就可以让多个进程监听同一个端口或者同一个进程中开启多个相同的serversocket。 但是这个选项要求所有的连接都必须设置SO_REUSEPORT选项，如果前面的socket没设置，后面再监听这个端口的时候就会报错。 来自于这篇文章的demo： server.py 12345678910111213141516171819import socketimport timeimport osPORT = 10002BUFSIZE = 1024s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)s.bind(('', PORT))s.listen(1)while True: conn, addr = s.accept() data = conn.recv(PORT) conn.send(b'server[%s] time %s\\n' % (os.getpid(), time.ctime())) conn.close() s.close() 模拟客户端请求十次 1for i in {1..10};do echo \"hello\" | nc 127.0.0.1 10002;done 无侵入实现端口复用已有的程序或者引用别人写的库时不方便修改代码或者不能修改代码时，就需要另外一种方式来实现。操作系统在执行系统调用时，会去寻找对应的.so库，在这个过程中可以执行拦截步骤。让系统先找到我们自己写的.so库，从而替换同名系统函数。通过LD_PRELOAD来进行拦截。 有人已经这个方式实现了一个，github地址。使用起来也非常简单，使用方式： 1234567891011git clone https://github.com/yongboy/bindp.gitcd bindpmake# 编译成功之后，在当前目录会出现一个libindp.so文件# 删掉server.py文件中 这行代码 s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)# 还是在当前目录REUSE_PORT=1 LD_PRELOAD=./libindp.so python server.py &amp; REUSE_PORT=1 LD_PRELOAD=./libindp.so python server.py &amp; lsof -i :10002 # 确认已经成功启动了两个进程 注意：SO_REUSEPORT在mac上的表现形式和linux上不一致 SO_REUSEADDR这个socket选项虽然字面意思也表示是重用地址，但它主要是用在服务重启的时候。当关闭服务时，连接就会处于tcp中的time_wait状态，如果不设置这个选项是不能成功启动服务的。 与SO_REUSEPORT详细对比可以参考stackoverflow的这篇回答，回答的非常好。下面的表格是引用这篇回答的。 SO_REUSEADDR socketA socketB Result --------------------------------------------------------------------- ON/OFF 192.168.0.1:21 192.168.0.1:21 Error (EADDRINUSE) ON/OFF 192.168.0.1:21 10.0.0.1:21 OK ON/OFF 10.0.0.1:21 192.168.0.1:21 OK OFF 0.0.0.0:21 192.168.1.0:21 Error (EADDRINUSE) OFF 192.168.1.0:21 0.0.0.0:21 Error (EADDRINUSE) ON 0.0.0.0:21 192.168.1.0:21 OK ON 192.168.1.0:21 0.0.0.0:21 OK ON/OFF 0.0.0.0:21 0.0.0.0:21 Error (EADDRINUSE)句柄复用与端口复用的对比在使用上明显是端口复用更加方便，甚至不修改任何代码就可以达到目的。由于这两种方法的原理不同，也导致了它们在实际应用上对性能的影响也不一样。 它们处理连接的方式如下图： 从图中可以看出句柄复用模式只有一个 listen socket （就是父进程创建的serversocket）来通知woker进程有连接来了，然后worker进程会产生竞争，最终只有一个worker进程能获取这个连接。优缺点： 优点 如果其中一个进程发生了堵塞，新的请求会分发到其它worker进程上，不会造成后续请求的堵塞 缺点 竞争带来额外消耗；有可能大部分请求都被一个worker进程获取到，造成进程间的负载不均衡 端口复用模式中，每个worker进程都会绑定一个serversocket，由内核来决定哪个worker进程可以接受新连接。内核的分发方式是循环分发的，即按顺序分发，非常均衡。 优点 减少了worker进程间的竞争，提升性能 缺点 如果一个worker进程发生了堵塞，那后续分发到这个worker进程的请求都不能被及时处理 两种方式各有优劣，实际场景中选择哪种模式应该结合具体业务来，模拟真实环境做个压测来比较一下。 更详细的对比可参考这两篇文章： Socket Sharding in NGINX Why does one NGINX worker take all the load? 引申问题判定一个服务否适合使用这种方法jvm进程一般都是多线程服务，如果这个时候再引入多进程模型的话，是否能提升性能，性能有提升又是为什么? 在我优化的这个案例中，性能是有提升的，而且提升的非常明显。我认为主要原因有以下几点： 由于cms垃圾回收器的机制，当进程占用的内存过大时，会导致gc时间增长。多进程能够缓解这个不足。 多线程势必要带来资源竞争造成的消耗，而并发越大竞争也可能越激烈，并且由于服务开发者的水平和业务的复杂度原因，很难去将这个消耗完全消除，而且要消除需要的成本也会非常大。多线程可以平分一台机器上的流量，就会变相的降低单进程内的竞争程度，因此会提升性能。 进程/线程调度由于这是多进程服务，需要考虑多进程和多线程在调度上的消耗。多进程是否会增加额外的消耗？ 线程上下文切换成本。同一个进程的上下文进行切换比不同进程的上下文进行切换消耗小，为什么？ 是否核越多服务的性能就越高？参考这个案例：fastsocket笔记 进程间通信方式都有哪些共享内存、管道、文件、网络、信号、信号量、消息队列。 参考文章 为什么nginx中一个进程中只有一个线程线程、进程的定义，多线程、多进程区别。 参考 linux core socket选项 SO_REUSEPORT fastsocket笔记 nginx architecture Why does one NGINX worker take all the load? SO_REUSEPORT fastsocket why jvm does not support fork Thread Affinity 进程调度 nginx 文件锁、自旋锁的实现 Linux 共享内存以及 nginx 中的实现","link":"/2020/01/01/%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1/"},{"title":"服务超时问题","text":"背景上游一个服务在调用我们服务的时候突然出现了大量的超时。首先怀疑的是docker机器又挂掉了，然后看了下服务的调用量监控,如下图。 从图中可以看出服务是在某个时间点突然没有任何流量（或者是服务本身问题导致不能上报监控指标）了。 服务之间是通过thrift进行通信的，thrift server的工作模式为TThreadedSelectorServer。 问题排查首先是登录到服务器上验证docker是否正常或者服务是否存活。随机找了几台服务节点，发现docker和服务进程都存活着，说明可能是服务本身出问题了。 使用tail -f error.log查看服务的错误日志，观察到一直在出现大量的异常。 123java.util.NoSuchElementException: Timeout waiting for idle objectat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:448)at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:362) 而且这个异常的出现时间点和监控出现异常的时间点相吻合。这会导致响应客户端请求的时间增长，导致客户端请求超时从而关闭连接。 通过 netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 查看对应端口的网络连接情况，单台服务器上有八万多个close_wait 连接。 查看tcp状态转换图可知客户端一直大量的新建连接和关闭连接，并且服务端没来得及处理关闭连接事件(调用close()方法)导致服务端出现了大量的close_wait。 大量close_wait造成的影响由于close_wait是系统层面的问题，比问题1容易定位些，所以先分析下问题2。 8万的close_wait连接说明服务进程中至少同时接收了8万个客户端的连接，其能造成的影响大致有以下几点： 连接不停的创建和关闭，会增大创建tcp连接的耗时 过多的tcp连接会占用一定的内存，如果还在服务器内存不够时，还会压垮整个服务器。单个连接占用情况可参考文章 服务处理tcp连接的线程模型会决定此服务是否能处理这么多的连接。由于服务使用的是nio来处理网络连接，处理接收连接和关闭连接事件是在单独的线程池，与处理业务逻辑的请求是隔离的，所以这个问题的只是影响了网络连接这块的，当前的这个服务来说不算特别大。 连接池问题由于当时的现场信息保留不完整，现在推测一下问题产生的原因。 java.util.NoSuchElementException: Timeout waiting for idle object是因为在从连接池里取新redis连接超时导致的。导致这个问题可能会有以下原因： 连接在用完之后没有及时释放，一直没有可用连接 redis集群突然出现问题，请求耗时增大、建立连接时间过长、网络抖动使请求总耗时增加 并发过大导致排队时间过长 当前的服务运行已经超过一个月，这次是突然几十台节点同时出现问题，并且流量没有变化。因此基本上可以排除1、3，最可能的是使用的redis集群出现了问题。而在redis集群出现问题又会不会导致问题1、3发生，由于没有足够信息，现在很难下结论。 结论看下当前服务的执行逻辑，来分析下连接池问题如何导致整个服务处于不可用的状态。 建立连接过程 处理请求流程图 上面这个流程图省略了业务处理逻辑，只展示了各个线程之间的交互关系。从流程图中很容易的可以看出在redis集群出问题时，服务中有两个逻辑漏洞： 当等待可用连接步骤出现了问题时，会导致thrift-server线程池中积累大量任务 并且在这个线程池中的任务会一直等待下去，不会被标记为超时 这就会导致了上游调用超时，并且后续的请求基本都会超时，这也就能解释为什么一个短暂的网络抖动导致上游持续超时。 解决方案有了结论，找到解决方案就非常容易了。 快速解决方案当前问题快速的解决方案是服务进行重启。 优化程序不管是作为客户端还是服务端都需要设置一个服务熔断策略，设置请求的超时任务在流程中应该在执行业务逻辑之前。服务在不可用的情况下，继续请求只会加大服务的负载，更难恢复，原因如下： 客户端在访问某个服务时出现了大量的超时情况时下，再持续的创建新连接发送新请求，也会严重的拖累自身。 服务端在发现服务基本不可用时，再继续接收新的业务逻辑请求时，会让本就超负荷运转的服务更难恢复过来。","link":"/2020/02/18/%E6%9C%8D%E5%8A%A1%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98/"},{"title":"特征平台（一）","text":"背景实时特征对运营活动是必不可少的，特征的质量也会对业务产生直接的影响。但是由于当前没有一个统一的实时特征接入和生产平台，导致特征的产出过程过于混乱，实时特征的质量也是不可控的。当前现象及主要问题有： 存在很多不同人开发的生产实时特征的工程，而且未来还会有更多。沟通、开发成本高； 很多特征的信息只存在于相应的工程代码中。没有被管理起来，信息缺失，难以追查特征逻辑并且其它项目很难复用； 没有质量监控。不能及时发现异常；计算逻辑问题难以排查。 这些问题已经对业务的扩展造成阻碍，因此需要对系统进行一次重构来解决这些问题。 注意：理解端到端对理解整个系统的设计非常重要，特征计算其实就是端到端的过程，将hive中的数据转存到fusion，mq中的数据转存到fusion、hive中，计算过程只是在时效上有区别，因此在设计整个系统流程的时候，不会着重区分实时计算和离线计算。 产品定位分析产品架构当前的产品架构为： 当前的系统虽然可以满足功能需求，但是在架构上没统一组织起来。模块间的耦合性太强，且特征的元数据都没有被统一管理起来。这些问题会使上线周期长、排查异常问题艰难，产品的扩展性也不是很好。 根据产品的特性进行模块划分和分层，将具有相同特性的项目划分成一个大的产品，再根据每个产品的功能进行分层。在划分后，每个产品都有了明确的定位，只需要专注于自己的逻辑。重构后的产品架构为： 特征是整个系统的核心数据，所有上层应用都要依赖特征来提供相应的服务，因此将所有与特征相关的功能聚合到一起成为一个系统，特征处理相关的功能尽量与其它系统在功能上解耦。 特征管理系统因此有一个完善的特征管理系统非常重要，在开发之前，先根据特征的生命周期来分析特征管理系统应该具有的功能。 从时序图中可以看出，特征的生命周期可以拆解为四个部分： 接入特征 对特征进行加工 使用特征 特征下线 实时特征和离线特征只是处理方法上有区别，但它们生命周期是一致的，所以在时序图中没有区分。 从特征的生命周期中可以看出特征管理系统必须能够管理特征的元信息，例如特征的接入、上下线；将特征按照某种逻辑重新加工；给上层应用访问特征提供统一的接口。 特征管理系统根据特征的生命周期可以将特征管理系统大致划分为元数据管理、特征加工、特征服务几个模块，每个模块只负责自己的逻辑即可，它们相互解耦，也利于扩展。本节主要划分每个模块的功能边界。 最重要的部分都用色块标示。 元数据管理元数据管理模块是整个系统的基石，它管理着所有特征的元信息。这个模块可以划分成四个部分: 特征需求接入负责对接用户的特征需求，录入接入特征需要的所有信息。在特征上线前的所有流程都应该在这个模块。 特征管理负责特征的状态、权限管理。特征的上下线，隐藏均通过这个模块处理。当特征需求评审通过之后，生成对应的特征加工任务。 元数据信息服务为外部提供特征的信息查询 质量监控用于展示特征的质量监控信息，实时特征和离线特征会有一定差别。当数据有异常信息时，及时发出报警，通知用户或者开发处理。并且有着可视化的监控指标信息，也能给客户安心的感觉。 特征加工特征加工应该算是整个系统的发动机，它负责将外部数据整合为特征。实时、离线特征的加工任务提供的功能是差不多一致的，都是获取一个数据源里的数据然后加工计算成需要的数据然后再存储到另外的数据源中，因此它们对外的接口基本是一致的。但是由于数据源的存储介质和时效性的区别，它们的技术架构也会有一定的不同。 特征计算／存储特征计算其实就是端到端的过程，理解端到端对理解整个系统的设计非常重要。分为实时特征计算和离线特征计算。特征的存储介质主要有hive、codis、fusion、es，特征的用途不同，存储的介质也会不同。 质量监控数据采集采集特征在计算过程中的监控指标，然后发送到元数据管理系统。 特征服务提供特征服务的能力，使外部系统能接触到特征数据。 特征选取上层应用在访问特征数据时，有可能需要事先选取下要访问的特征、访问方式，因为不同的存储介质、不同的应用，访问方式也不一样。 特征查询使外部用户能直接查询特征，提供的服务方式可以是多样的。hive中的数据离线查询，fusion和codis通过微服务查询或者sdk方式。 实时特征加工实时特征与离线特征的主要区别在于时效性，因此实时特征加工任务最需要考虑的特性是低延迟。Flink是一个高效的流计算平台，当前模块是基于Flink来实现的，会建立多个Flink任务来完成特征计算。Flink也是支持批处理的，后期部分离线特征任务可以考虑也使用Flink来完成。 特征计算过程分成两个步骤： 任务管理 当特征新增、逻辑修改、下线时，会生成一个任务。任务管理模块会定时读取任务，来触发相应的动作。为了加工特征，可能会产生很多独立的Flink任务，任务之间相互隔离可以使任务之间不会相互影响，提高特征的稳定性。但是如果为每个特征都生成一个Flink任务，是不必要并且会浪费大量的资源，当特征的数量过多时，这种方案也是不可行的。因此需要合并部分特征任务，减少任务数量。 合并任务主要依据于几个因素： 用户指定多个特征共享同一个任务 根据特征的source、sink配置判断是否能进行合并，例如：同一个topic的产出的特征并且存储介质一样，则可合并。 如果source的qps较低，即使source、sink配置不一致也可合并到一个Flink job中。 在任务合并之前需要通知并让用户来确认。 执行Flink任务 本模块可以分为打包和计算两个过程。打包是自动根据特征的元信息在服务器上生成个jar包，并提交到公司的Flink计算平台。 计算过程也较为简单，Flink任务在特征消息流之外还会额外消费任务配置流，任务配置流的主要作用是通过下发与任务相关的配置，例如：计算逻辑的修改，再通过广播的形式将配置发送到整个任务中。由于任务事件较少，不会对Flink任务的计算流程产生影响，因此所有Flink任务可共享同一个任务配置流。 如果DB能支持幂等性操作，则数据是能达到强一致性的。不过大部分DB都不支持，因此数据最终能达到什么样的一执行是和当次任务的配置（存储介质、计算逻辑）相关的。 标签系统的实时特征都是以hash的数据结构存储到redis中，操作为HINCRBY和HSET，在这个场景下可以结合Flink的checkpoint来达到数据的最终一致性。 质量数据采集实时特征的质量数据分为两部分：Flink任务在执行过程中产生的指标，实时特征准确性校验数据。 Flink任务监控指标 通过Heartbeat可以计算每个任务中每个节点耗费的时间，系统的可用性。参考文章。 这一块再进一步，可以让用户通过配置的方式来监控某个用户在某个时间段内的所有相信，包含对应的消息、产生的特征值，用于验证和排查问题。 准确性校验实时特征在生产出来之后，需要进行准确性校验，可以通过与对应的离线特征进行比对来校验数据是否准确。 由于这块的成本较高，可以选择对重要些高的特征进行校验。 特征服务这个模块主要是为了统一外界访问特征的方式，统一的访问方式也使外界可以方便的接入特征。 模型与特征管理平台之间的交互流程 特征选取 在用户能真正访问特征需要校验其是否有权限，并且特征是否能满足对应的用途，如果不满足可以提交个特征加工任务，将特征重新加工。例如模型服务在上线时需要选取相应特征，如果某个特征只存在于hive中，那就需要将这个特征录入到fusion中，这时就可以新建个离线任务来执行。 不同的用途对特征的访问方式也不一样，生成的配置也不一样。例如：模型服务用途生成的是一份特征配置；而通过api来访问特征生成的就是一份访问说明文档。 特征查询查询方式可以有多样的，离线查询、API、SDK方式。 以模型服务举例，模型服务一次访问的特征过多，为了优化性能，需要通过提供的SDK和建议的方式来查询特征。其中模型服务在启动时，即可以通过访问特征管理系统来更新配置信息，也可以通过生成的配置来获取特征元信息。SDK的使用方式可以参考下醉酒模型的流程。","link":"/2020/03/08/%E7%89%B9%E5%BE%81%E5%B9%B3%E5%8F%B0%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"知识体系","text":"在述职分享时发现自己还有很多不足的地方。需要一个长期的规划和系统性的方法来帮助我查缺补漏、确定长期目标，以更好的发展。 本文的目的在于找到构建自身知识体系的方法，主要是从知识体系的作用、如何看待问题、怎样去划分知识体系以及如何填充知识体系框架来写的。 作用当需要掌握的知识越来越多时，对个人的挑战也越来越大，如果不把这些知识体系化的组织起来，不仅很容易遗忘一些知识点，个人水平也很难提高。 理解更加深入 易于扩展新的知识点 查缺补漏 快速掌握新的知识 全局视角，扩展思维方式 创新 看待事物的方式事物：某项技术、业务系统、知识点。 大部分事物都是因为某种目的产生的，它们都有自己的核心价值。如果能够清楚它们的来龙去脉，就会更加容易推断和了解它们的实现原理，之后也更容易灵活运用，或者当它们不符合需求时，也能洞见出问题所在，从而有目的改进或者创新。对一项事物可以从这三个方面来去了解： 面向的用户是谁 事物存在的意义必然是要依托于“用户”的，只有清楚了事物是为谁服务的，才能抓住某项事物的演化方向。 主要解决什么问题 这个事物的出现肯定是要满足用户的某些需求，它能解决的问题也就是它的核心价值所在。明白了用户的核心需求之后，用当前掌握的知识点尝试推演解决方案，再与已有的方案进行对比，不仅能完善思考方式，也能更加容易的理解这个事物。 体系中的定位 事物即要缩小来看全局，又要放大看它的局部。全局是要理解它的业务和功能边界、核心价值，局部是要看它的实现原理。以业务系统来说，全局视角能接收更多的信息，有助于把握系统的进化方向和架构设计；局部视角需要良好的工程能力和产品思维，才能实现出好用、易用的系统。在业务理解、工程能力、产品思维的基础上持续的创新，才能持续的扩宽业务边界和优化系统。 用户的需求是会变化的，应该以发展的眼光来看某项技术或者当前的业务系统，随着用户的变化而变化才不会掉队，甚至推动外界的变化。 体系划分体系划分的目的是用具象化的方式来将自身的知识体系表达出来，把每个知识点按照它们之间的关联关系有组织的构建起来，以具象化的方式展示出来，再有针对性的进行填充。 工程、算法、产品、运营都是为业务服务的，并且相互配合来达到最终目标，它们的不同在于为了达到最终目标使用的实现方式不同。我认为可以将个人的工作能力分为三个部分： 专业能力 立身之本，脚踏实地，才能仰望星空。 业务能力 只有理解了业务，才能将个人的专业能力发挥出更大的价值。 创新能力 它会决定个人的上限在哪。结合专业知识和对业务的理解，洞察当前业务可以改进的地方或者扩展业务边界，为集体和个人带来更大的收益。 专业能力和业务能力的边界和划分，是会随着时间和工作经验的积累变化的。 工程知识体系我主要从事工程方面的工作，工作方向偏向于业务一些。 知识体系划分为三个层次，架构设计、知识面、基础知识。随着个人的成长，每个层次在整个知识体系中的占比也会发生变化。 建模抽象建模方法贯穿了这三个层次，它主要考察的是我们对事物的认知能力。针对某个问题建立抽象模型，然后基于模型分析和评估，推断它的变化和发展规律。能够帮助我们快速的掌握和发现问题。 基础 知识面 架构设计我认为架构设计是一种通用的非常抽象的指导思想，它主要在与指导我们如何做事，最终的体现就是我们能依托于底层知识来解决复杂的系统问题。这种思想可能在多个领域是通用的，例如：软件工程、建筑工程。 工作中的三个要素：事、人、物。事：要完成的目标，可以是一项业务指标、也可以是一个具体的业务系统。人：与事情相关的人，这些人的角色可以分为用户、需求方、实施方。物：为了完成事情用到的工具，工具既可以是具体的东西，例如开发框架、开发平台，也可以是沉淀下来的方法、经验。 系统架构设计的核心在于对业务的理解、沟通表达、对相关方向技术的掌握程度。不同的阶段有不同的侧重点。 参考 数学建模——五步方法","link":"/2020/04/03/%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"title":"流式计算","text":"设计一个流式任务的时候，需要从几个方面考虑： 计算语义 数据一致性 业务场景对数据一致性的要求，不同的业务场景要求的技术方案可能也不一样","link":"/2019/12/12/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"},{"title":"特征工程升级实践","text":"背景当前团队内有多个围绕着“特征”建立的系统，有特征系统、用户画像系统、分析门户等。在业务发展初期，这些产品都是紧贴业务的发展而建设的，系统功能模块和沉淀下来的特征也是基于运营活动、业务单元而垂直开发的，给后期带来了巨大的维护和开发成本。 而且随着数据的规模和系统功能复杂度的大幅增加，现有系统架构已经不能高效支撑运营和各业务方的使用，因此需要对现有的产品和技术架构做一次升级。 本文从分析现状出发，围绕着平台的价值属性（工具、内容）来解构当前的根本问题，找准升级方向，最后通过调整产品架构、优化或者重构技术架构来完成这次升级。 现状用户对平台的反馈主要集中在两方面： 对接流程繁琐，需求开发周期长； 数据很难找，找到了也不敢用，用了也提心吊胆。 效率问题从几个case来看下当前的效率问题。 当前的产品架构： 当前的系统虽然可以满足功能需求，但是在架构上没统一组织起来。模块间的耦合性太强，且特征的元数据都没有被统一管理起来。这些问题都会使上线周期长、排查问题艰难，产品的扩展性也不是很好。 特征接入过程，以画像系统为例： 当运营需要使用一个特征时，需要和多个团队对接，接入特征耗时基本都是以周为单位的，而理论上是可以达到以小时为单位的。 业务方认为人群判定不符合需求时，排查问题的流程： 事后排查，并且都需要画像团队产品、RD介入，如果是数据源的问题还需要业务方去推动数据生产方优化。当涉及的团队非常多时，排查和解决问题的耗时对运营来说简直是不可想象的，特别是重大运营策略使用的数据有问题时，甚至还会造成资产上的损失。经过总结分析，当前遇到的大多问题都是可以通过技术的手段来自动化定位、告警、提前解决的。 内容问题当前大多特征接入都是以ft、个人为单位的，带来各种不同的烟囱体系，业务方如果想要夸业务域使用数据也是比较困难的。当特征规模增大以后，数据口径对不齐，特征质量参差不齐的让数据使用方提心掉胆，质量问题也难以追溯的问题就非常突出了。 总结从当前现状和用户的反馈来看，可以看出当前平台面临的两个根本问题: 效率 对用户来说，使用平台过程繁琐，流程也过长；平台方来说，重复开发较多； 内容 当前平台只是做为工具来对外提供一些功能，没有形成有效的治理数据体系，严重不符合用户的期望。 从数据生产侧和数据消费两个角度可以总结以下痛点： 数据消费侧： 接入特征周期过长 信息不完整，使用上容易出错，经常重复沟通，效率低； 对大规模的特征没有建立良好的规范体系，运营在使用时效率低，一些无关的特征会形成很大的噪音； 没有质量保障，都是在发生事故时才能感知到数据异常，且追踪问题耗时耗力。 数据生产侧： 有多套功能类似的系统，开发和维护成本过大； 系统之间的功能存在耦合，且没有被统一整合起来，一次改动可能会涉及到多个系统，排查问题艰难； 特征缺少统一规范约束，经常会造成重复开发，数据冗余，且同词不同义的情景下沟通成本过大； 没有一个统一的数据质量管控体系，不能做到事前通知，收到用户较多的负面反馈； 数据冗余会造成计算和存储资源的浪费。 如何做定位除了现有的问题，还需要考虑以下几个方面： 平台的价值属性定位 平台在之前的定位基本上只是一个工具属性，仅仅是为了提高运营、算法同学的工作效率，虽然接入大量的特征，但是并没有很好的管理和利用起来，这样已经不满足当前的业务形式了。因此为了让平台跟上业务的需求，还需要提高平台在内容方面的价值。 特征来源 当前平台内的特征大多都是业务方接入近来的，少部分是特征挖掘组输出的。但是公司内的各个数仓团队在实际上还沉淀了大量的指标（特征），这部分特征如果能直接接入到我们平台中的话，会极大的丰富我们平台的数据内容。 团队合作 当前一个特征需求需要业务方自己去跨团队来推动完成，效率是非常低的，如果我们能和数据、算法团队建立良好的合作机制，打造一个一站式特征需求平台，将会极大提高业务方的工作效率。但这就涉及到我们平台如何向数据、算法团队反馈收益。 因此我们平台的最终定位是加强与数据、算法团队的合作，尽量打造一个一站式的特征、画像平台，不仅要大幅提高业务方的工作效率，还需要为其提供高质量的特征数据，让用户可以大胆、放心、高效的使用我们平台上的任何功能和数据。 目标拆解根据前面的分析，我们这次需要做的事情可以分解为三个部分，数据治理、产品架构、技术架构，也就是以下几点： 通过数据治理来制定数据规范、保证数据质量； 然后再通过调整产品架构清晰划分每个产品的功能边界，提高数据生产方和数据消费方的工作效率； 最后再以设计良好的技术架构，保证产品功能落地，性能上高效的支撑系统运转。 数据治理平台内当前的所有数据都是由各个数仓团队的数据工程师（DE）、业务方RD、算法工程师提供的特征，然后再以产品的形式提供给用户使用，不涉及到数据清洗和复杂的数据建模。因此我们需要解决的问题主要是： 制定一套业务标准来规范特征的接入、使用、上下线； 建立特征的质量监控体系，并且能够做到做持续监控，事前告警，事后追踪复盘。 业务标准业务标准主要针对的是特征的管理过程，主要目的是使各业务方对特征的认知达成一致。标准的定义主要参考的是数仓团队制定的标准，后期的更新迭代也需要尽量要与数仓团队保持一致。 口径规范口径规范的目的主要是为了消除特征的二义性，影响的主要是接入、生产环节。使用基础数仓团队已经沉淀出的特征规范体系，并且保持一致的一个好处是可以将其沉淀出的大量特征快速接入到我们平台上。主要解决了以下几个问题： 口径不一致的问题，例如：同名不同义； 重复计算和接入的问题，节省存储和计算资源，并且也节省了用户等待需求完成时间和开发人力。 将一个特征拆分为业务板块、数据域、业务过程、原子指标、实体类型等，多个特征也可以通过四则运算生成新的复合特征。拆解过程如下图： 业务板块 指的是事业部，可能有由多个业务线组成，如网约车，包含专车、快车等多个业务线。 数据域 业务过程的抽象集合，例如，交易域、流量域。 业务过程 是指企业的企业活动事件，业务过程是一个不可拆分的事件，例如交易过程中的发单、完单事件。 原子指标 基于某一业务时间行为下的度量，是业务定义中不可再拆分的指标（比率等指标除外），具有明确业务含义和业务完整定义的名词。例如 发单（业务过程）+ 数量（度量 ） 实体类型 指的是特征依附的实体，例如乘客、司机、商家、订单、骑手等。 特征 衍生特征指的是普通特征，例如乘客历史截止到当日网约车完单数。复合特征指的是两个或多个特征经过四则运算生成的一个新型特征，例如：乘客历史累计网约车完单数 （实时） = 乘客历史截止到当日网约车完单数 (离线) + 乘客当日网约车完单数 （实时） 特征分类业务分类的目的是帮助平台的用户更快的找到所需要的特征，影响的主要是应用环节。通过建立三个并行的分类体系，业务分类、内容属性、活动主题，可以使用户可以更加方便的找到所需数据： 业务分类 其实就是指标拆解的那个过程，可以换一种用户更容易看懂的说法来描述 内容属性分类 这是特征固有的属性，与业务线、活动主题无关，例如：基础属性（性别、年龄），行为特征（打车次数）； 活动主题 运营在制定实际的活动策略时，会跨业务板块、数据域去使用特征，并且活动策略的目的可以分为多种主题，例如：拉新、增长、沉默召回等等。同时一个特征也可能会适用于多种主题，因此特征和主题是多对多的关系。主题的定义是与业务系统、活动策略紧密相关的，特征归属的主题可以适当的下放给用户来设置。 建立质量体系通过建立特征的质量体系，量化特征的质量，会影响特征的全部环节。主要目的有以下几点： 接入前评估，保证接入到我们平台的特征质量都是合格的； 接入后持续监控，当特征质量发生波动时，给相关责任人发出告警；如果低于某个阈值，阻断后续使用，并向所有特征使用方发出告警，从而可以联合用户推动数据生产方改善数据质量； 保证数据一致性，在不同的应用场景下，存储引擎也会不同，因此还需要监控不同的数据库中数据是否一致； 事后复盘，完善的监控体系详细的记录与事故相关的所有信息，为事后快速定位问题提供有力的依据； 监控长时间不用的特征 可将其下线或者冷藏，节省资源。冷藏还可以再次快速启用，不需要重复开发。 通过以上几点也可以极大的提升我们平台的口碑。 根据监控方式的不同，将质量监控分为以下类型： 基础质量评估 会覆盖大部分数据，评估结果代表着数据的健康程度。监控指标有：主键重复、枚举值、空值，表行数，存储文件大小，数据源的监控等。 生产任务监控 生产任务的健康程度也会影响到数据的健康，因此也需要健康它的运行情况。监控指标有：启动时间点，运行时长，是否发生异常。 数据一致性 在不同的应用场景下，数据的存储引擎也会不同，同一份数据可能会存储到多个数据源中，因此还需要想办法核对数据的一致性。例如：画像系统中，离线数据会同时存储在hive、es、fusion（kv数据库）；根据更新周期的不同，特征也分为实时（存储在redis）和离线（存储在hive），它们的计算口径虽然相同但数据来源不同，因此通过校验它们的值是否一致，也可以监控口径是否发生变化。 数据运营主动的向外输出，提升平台的知名度、用户的活跃度、特征使用量。具体措施有： 收集优秀案例，并向用户展示特征的用途、玩法； 向用户推荐可能想要使用的特征，免去频繁查找之苦； 通过监控体系，根据使用次数、相关度、特征质量综合排序，将高质量的特征优先推给用户，并且主动下线、优化质量不好的特征。 产品架构 当前的系统虽然可以满足功能需求，但是在架构上没统一组织起来。模块间的耦合性太强，且特征的元数据都没有被统一管理起来。这些问题都会使上线周期长、排查问题艰难，产品的扩展性也不是很好。 顶层架构本次调整，各个产品对外的主体功能不会发生改变，主要是根据产品的特性进行模块划分和分层，将具有相同特性的项目划分成一个大的产品，再根据每个产品的功能和依赖关系进行分层。在划分后，每个产品都有了明确的定位，只需要专注于自己的业务逻辑。调整后的结构为： 特征平台 将所有与特征管理、生产、监控的聚合到特征平台上，向上屏蔽掉了与特征相关的所有细节，简化了上层应用系统的接入方式。 画像系统 将与特征接入、生产相关的功能剥离到特征平台，只通过特征平台使用特征，主体功能不变。主体功能有人群筛选、人群分组、人群下载。 算法模型系统 将与算法模型相关的模块抽象成一个系统，主要负责模型的上下线，版本迭代。 特征平台相比前两篇文档，本次文档主要只是将功能模块依据这次的规划重新组织了一下。落实到技术架构上变化很小。 模块划分的分析过程就不再详述了，下面是这次重新划分后的架构。 画像系统 当前所有用户是共用一个大空间的，接入的几千个特征和用户创建的人群都是堆在一起的，这会造成以下几个问题： 信息干扰； 因为功能需要，所有特征都是存储在同一个ES索引，生成速度慢； 权限控制较为复杂。 因此将画像系统按照业务线、FT为单位划分成多个空间，每个空间只包含与空间主题相关的特征、操作数据。拆分后的优势： 减少信息干扰，让用户更加聚焦； 可以提高数据处理的并行度，加快数据处理速度； 逻辑上隔离，各个团队之间的数据不会相互影响。","link":"/2020/06/26/%E7%89%B9%E5%BE%81%E5%B9%B3%E5%8F%B0%EF%BC%88%E4%B8%89%EF%BC%89/"},{"title":"特征平台（二）","text":"整体架构在上篇文章中主要是叙述了特征管理系统的顶层设计，从业务的角度对其进行了产品定位，根据它的定位将其划分为元数据管理、特征加工、特征服务三个功能模块，并简要的描述了实时特征计算的大致实现流程。本文再细化特征管理系统的实现。 用例在做详细设计前，首先需要清楚用户如何与本系统进行交互，根据两者之间的交互动作来分解系统功能和做技术选型。 从用例图中，可以看出每个子模块需要提供的必要功能。在知道了模块的基本功能之后，再根据一个实际的案例将各个功能模块串联起来。 用户提了一个接入实时特征的需求，经过需求评审，可以接入特征管理系统； 然后触发特征上线动作，特征加工模块监测有新任务来了之后创建实时特征任务； 采集实时特征任务在执行过程中的监控指标，将监控数据发送到Druid中； 元数据管理模块中的监控子模块会将监控指标可视化的展示出来，并且根据规则来判断是否要触发任务异常报警； 模型服务通过特征的元数据找到其对应的集群地址来获取特征值。 架构图功能架构图 数据处理架构图 实体设计核心流程上的实体对应的ER图如下： 根据用例，可以得出以下必要的实体： 需求 记录用户提交的需求的详细信息 特征 这是系统中的核心，整个系统都是围绕着特征提供服务的 动作 操作特征出发的动作，特征加工模块会监控这个动作来提供相关的信息 任务 记录建立的计算引擎任务 特征计算过程可以看成是一个管道，原始数据经过经过一个管道之后变成另外一种数据，因此系统要再抽象出一个必要的实体： 管道 包含有源端、目标端、计算逻辑。由于同一个特征可能存储在不同的介质中，因此它可能会存在多个管道。管道也是本系统最核心的概念。一切操作都是基于管道的。 同一个特征可能存储在不同的DB中，例如：乘客大宽表，乘客特征的存储介质有hive、es、fusion，在这种情况下就需要建立多个管道来执行计算任务。一个计算逻辑也有可能会产出多个特征，例如：一个topic会产出发单数、完单数标签，因此管道也可以同时对应多个特征。 当特征上下线时，实质就是触发其对应的管道产生一个动作，然后根据这个动作再生成一个对应的计算任务。由于可能会将不同的管道计算逻辑放置到一个计算逻辑去执行，因此一个任务对应了多个管道。 ER图中只包含了执行一个特征计算任务所需要的所有信息，在实际的系统中，还会有额外的信息，例如：添加／更新时间，创建人等等。根据这个ER图就可以开发出系统的核心功能了，其它的辅助功能或者辅助信息可在实际开发时根据需要添加。 其它实体 数据源 记录数据源的详细信息 关系表 记录实体之间的关联关系 任务历史 记录任务的执行历史 特征变更表 记录特征的变更历史 元数据管理元数据管理的主体功能是特征上线、特征质量监控，其它的模块都是对主体流程的辅助。每个模块的功能边界可以参考文章（1）。 其大体功能如下： 特征上下线由于特征的计算被抽象成管道，因此特征的上下线本质上就是对管道的上下线，对特征的所有动作都会反应到管道上。特征上线就是将管道合并到正在执行中的任务或者创建一个新的任务，特征下线就是从正在执行中的任务中抽离出对应的管道。 因此特征上线概念更严格一点就是（特征，管道）二元组的上线，将特征对应的管道部署到线上。例如 （用户完单数，hive-fusion）的上线，不会触发（用户完单数，hive-es）的上线。特征下线同理。 特征质量监控这个模块是用于保障特征质量稳定性的，良好的特征质量才能向外保证特征是可靠的，才能将特征和平台推广出去。 以可视化的看板形式展示与此特征相关的质量数据 特征任务特征加工任务本质上就是将一个或多个相应的管道合并成一个计算任务。 特征动作解析整体流程如下： 这个流程的重点是查找可以合并任务，前提是理解任务与管道之间关系。他们之间的关系用以下过程表达 123456789jobs = {job_i | 0 &lt;= i &lt;= n}pipelines = {pipeline_j | 0 &lt;= j &lt;= m} pipeline之间有共同点n &lt;= mjob_i = {pipeline_j_k | 0 &lt;= j_k &lt;= m}，一个job是多个pipeline组成，并且一个pipeline只能属于一个jobjob和pipeline的映射关系是：有共同点的pipeline会被分配给同一个job。pipeline先找具有共同点的pipeline，然后再找到对应的job。如果job存在，job对应的pipeline集合就增加一个pipeline。否则就新建。 判断管道是否能进行合并： 管道如果设置为独立任务，则不会执行合并 同一个组下的管道会进行合并 根据源端、目标端、计算逻辑判断是否能进行合并，例如：(hive db.table1, hive db.table2, select {tag} from table) ，这种情况下就是可以合并的。如果源端类型为消息队列的话，一般都是可以执行合并的。 如果源端类型为消息队列的话且qps较低，一般都是可以执行合并的。 执行引擎的选择可选的执行引擎： flink 主要是用来处理实时数据，即管道的源端为mq spark 用来处理离线数据 jvm进程 主要执行一些测试，例如在计算任务正式上线前，可通过jvm进程的方式来验证计算逻辑是否正确 flink引擎大致流程如下： 主要是分成两块，任务打包，计算逻辑的封装 由于计算任务的多变性，这里会提供一个基本的生产样例，主要是用来迁移torrent的。 后面会根据需求来提供更加丰富的样例。","link":"/2020/03/10/%E7%89%B9%E5%BE%81%E5%B9%B3%E5%8F%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"系统工具","text":"进程／线程 查看所有线程数量 ps -eo nlwp,pid,args --sort nlwp 监控 watch -n 1 'ps -eo nlwp,pid,args --sort -nlwp | head' 查看进程线程数量，每个线程的cpu利用率 top -H -p pid tcp 查看tcp 半连接和全连接队列情况 netstat -s | egrep &quot;listen|LISTEN&quot; ;ss -lnt 统计tcp连接各个状态数量 netstat -n | grep &lt;port&gt; | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 负载 mpstat 查看线程的上下文切换次数 pidstat -p 36913 -wt 1 查看总的上下文切换 vmstat watch -n.5 grep ctxt /proc/$pid/status 参考 figure-out-which-process-forks-too-many-threads system call and context switch process-threads-context-switch tool process tools linux性能速查-CPU上下文切换 工具指标 案例","link":"/2020/03/27/%E7%B3%BB%E7%BB%9F%E5%B7%A5%E5%85%B7/"},{"title":"服务性能优化过程","text":"背景组内有好几个线上服务，除了业务逻辑不一样，请求处理过程基本上都是一致的。这些服务的执行逻辑都非常简单，但是有几个问题: 单机QPS很低，需要很多台机器 99分位耗时比理论上的长很多 在单机qps达到上限时，服务器的负载却非常低 上面的这些问题在财大气粗的公司面前都不是问题啊，性能不够加机器！性能不够加机器！ 上游再一次过来反映超时问题时，为了壮年程序员的尊严，这次刚好有时间决定不再单纯的加机器，要把这个程序优化一下，用一次屠龙技。 技术栈： 语言：java 通信方式：thrift，server模式为THsHaServer redis客户端：jedis 模型：xgboost 执行环境：docker，8核 8G 缓存集群：通信协议为redis，但实现方式和redis不一样优化后结果 优化过程想要提升程序的性能，首先需要找到程序的瓶颈所在，然后有针对的去进行优化。 流程分析优化前流程图 这个流程图省略了业务处理逻辑，只展示了各个线程之间的交互关系。构造redis请求的那部分逻辑，其实使用了两个forkjoin线程池，但它们两个的逻辑非常类似，为了简化把它们合并到一个流程中了。流程图中有两个深红色的步骤，很明显的看出两个明显能影响性能的地方。 结合服务的执行环境，可以总结出以下问题： 线程数过多 由于操作系统对线程的抢占式调度，线程频繁的上下文切换会带来几个问题： 系统指令执行时间增长，对应的指标值为cpu.sys，造成cpu执行时间的浪费 单位时间内分配给执行用户态指令的时间减少，对应的指标为cpu.user 综上可看出，其不仅使机器负载升高，也会使执行单次任务耗时增长 两个阻塞 流程图中标红的两个步骤在执行时都会阻塞线程。其中thrift-server是在阻塞等待所有的redis结果，forkjoin中是在阻塞等待redis返回结果，网络通信使用的是同步io模式。 线程从运行状态切换为阻塞状态时，会发生一次线程上下文切换并且线程需要等待被重新调度。这是在操作系统层面的影响。 假设服务同时接收到40个请求，从流程图中可以看出，此时服务最多同时能发送86个redis请求，而要想让服务能通时执行40个任务，则必须要同时发送2000 = 40 * 20个redis请求。如果将forkjoin的最大线程数调整到2000明显是不可以的，实际在这个服务最初的版本中是没有限制forkjoin的最大线程数的，所以在服务负载升高时，服务器开启的总线程数也一直在飙升，此时就会收到疯狂的服务器负载报警，且请求调用耗时也非常高。 由于阻塞线程的操作，要想增大程序的并发就只能多开启线程，而线程数量多了就会影响程序的性能 使用同步io方式来发送redis请求，同时发送的redis请求数量非常有限。这不仅严重拉低了qps，而且也使99分位耗时增长。 任务无限期等待 从上面也可以看出任务没有超时的限制，可以无限期的等待下去。这会造成以下影响： 如果客户端没有设置超时时间，可能会无限期等待下去，可能会拖垮客户端。 如果客户端因为超时而取消了这次任务，那么这个任务再被执行是没任何意义的，而且还会挤占其它任务的执行时间，甚至造成程序的雪崩。 总结： 从前面的分析可以看出，现在程序的瓶颈主要是查询redis过程，其次是thrift-server中线程的阻塞逻辑。 流程优化由于程序的JVM GC监控指标在正常范围内，并且改动架构还需要重新观测GC情况，所以先做程序架构上的改造。 查询缓存过程 这主要是一个网络优化的过程，业务层通常是从以下几点入手： 减少通信次数，即合并多个请求成一个网络请求。因为我们公司的缓存集群实现方式和redis不一样，使用pipeline、mget对性能提升有限，甚至降低。 使用异步IO，不仅能减少使用的线程数，而且能增大同时发送的redis请求数量。 当前使用的redis client是jedis，用一个开源的异步redis client lettuce代替，它的底层使用的neety。 thrift-server 使用AsyncProcessor来将其改造成异步。 任务增加超时限制 给最上层的CompletableFuture设置一个超时逻辑，当任务执行时间超时就取消CompletableFuture的执行，并将这个取消通知逐层传递下去。 流程优化后的流程图： 上面这个流程还是有几个薄弱环节，在一次偶发事件上，出了一次问题，相应文章。 有了新的流程图，代码改造就纯粹是一个按图实现。为了减少改造过程中引入新的bug，要秉持一个原则：尽量复用之前的代码。 压测压测工具使用的QA组提供的工具goperf2，使用jprofiler监测jvm进程执行情况。jprofiler这个工具非常重要，通过它的可视化界面能帮助我掌握jvm进程中所有线程的执行状态，然后根据这个状态再去调优一些参数。 测试过程非常枯燥，但看着指标的提升，又非常有成就感。 初步的压测结果证明了前面的优化思路是正确的，还需要再填一些小坑道路才能平坦。压测过程中发现的问题： 当在压测时，服务给出空响应耗时特别长。空响应是不做任何业务逻辑，收到请求就直接返回。 原因：selector线程处理不过来了。 压测程序跑了一会之后，服务器上出现大量time_wait，就提示不能在分配连接地址了。典型的tcp面试题，是因为端口不够分配或者进程的句柄数达到最大。 服务器上也出现了大量的time_wait。lettuce连接池参数问题，使连接不停的关闭。 单次访问耗时不稳定。jvm类加载过程、连接池初始化、线程池初始化的问题。 连接池borrowObject耗时很长。参数优化。 正则表达式性能问题，原先代码逻辑耗时 java stream耗时 hashmap不停的新建 最终压测结果： 类型qps平均响应时间90分位95分位99分位99.9分位优化前只取45个实时特征29516.7617105055优化后只取45个实时特征117812.13349优化后只取45个实时特征，三个进程172182.2533711优化前预测17741419356168优化后预测27029.211121417优化后预测 三进程47058.513141724 压测中用到的命令查看tcp 半连接和全连接队列情况 netstat -s | egrep &quot;listen|LISTEN&quot; ;ss -lnt 统计tcp连接各个状态数量 netstat -n | grep &lt;port&gt; | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' jstack线程数和jvm进程开启线程数不一致经过对程序架构、代码实现的优化，新版的程序终于上线了。上游也不再反馈超时了，机器负载也不高了，饭量也大了。 但是某天再看监控的时候，发现JVM进程竟然开启了800多个线程，远远超出理论上50个以内的线程数量。 就简单的说下分析过程和原因吧。 定位过程验证监控是否出错 通过这几个命令来查看进程的线程状态和数量, 发现和监控一致，证明监控没有问题。 12345ps -eo nlwp,pid,args --sort nlwptop -H -p &lt;pid&gt;ps -o nlwp &lt;pid&gt;ls /proc/&lt;pid&gt;/task | wc -lls /proc/&lt;pid&gt;/status | grep thread 验证jstack问题 通过查阅jstack文档发现，使用jstack &lt;pid&gt;只会输出受jvm进程管理的线程，只有jstack -m &lt;pid&gt;才能输出.so库开启的线程，但是jstack -m打印的堆栈信息不全。 定位开启非jvm线程的代码 打印进程堆栈，gstack &lt;pid&gt;，发现除了jvm线程之外，大多线程的堆栈信息都是一样的且都包含了libomp.so。这个库是用于c/c++中的多线程库，相关资料openmp。也就说程序中有代码调用libomp.so开启了大量的额外线程。 而开启线程肯定是要经过系统调用的，通过strace命令可以跟踪到是哪个线程开启的libomp线程。在这个过程中，需要jstack打印出jvm线程堆栈，通过比较线程id可以准确定位具体的jvm线程。 最后定位到是算法组同学给出的模型jar包调用的。 xgboost 定位到算法组给出的模型jar包的问题之后，开始浏览他们的代码，最后发现根源问题在与xgboost。相关代码如下图。 jvm线程再调用Predict方法时都会开启omp_get_thread_num个线程，但是一个jvm线程只会开启omp_get_thread_num个线程，最终的总的xgboost线程数量刚好与count(jvm thread) * omp_get_thread_num吻合。 除了xgboost c库的问题之外，官方的java库也存在一个问题。在使用xgboost的时候，一般只会创建一个Booster对象，然后多线程调用booster.predict()方法，最终都会调用一个同步方法。 这个方法是个同步的，在多线程调用的时候，同时只有一个线程能执行预测方法，其它线程进入到阻塞状态。在没有竞争的情况下，predict()方法的耗时在1ms左右，当竞争激励时，方法耗时甚至能增加到20ms，且没有上限。这在对请求耗时敏感的业务场景下，是一个不可接受的问题。 解决方案解决方案有两种： 程序在启动前添加环境变量，例如OMP_THREAD_LIMIT=1 control.sh start。用来限制omp开启的线程数量。 使用纯java实现的xgboost库来替换xgoobst的官方库， 这个服务还遇到过一个堆外内存问题，不过定位过程足够再写一篇文章了。 进一步的优化至此，对服务的优化效果已经达到预期。但其实还可以做出进一步的优化，但耗费的精力就非常大了。 对程序优化的本质是考察程序员对操作系统相关知识点的理解程度： 进程／线程原理，线程是操作系统执行的最小单位，如果真正的理解这句话，那么再做任何架构方面的优化就非常容易了 IO原理，相关知识点：存储、总线、DMA 线程之间的同步最终也会反映到线程的执行 更近一步的优化思路，由于多线程之间势必会涉及到资源的竞争，这一般是通过锁来进行同步。那么找到对程序中的同步点并且尽量消除这些同步点对程序的性能会有非常大的提升。优化同步点是一个非常关键但也非常耗费精力的事情。 程序运行起来，那么肯定就是要接受输入并且处理再输出结果，那么追踪一条请求的数据走向，那么就能找到在完成一次请求的过程中，都有哪些同步条件发生了。而一个多线程程序是以线程池单位来完成整个程序的执行逻辑，线程池内的线程一般执行的是同样的逻辑。先粗粒度的来分析数据的同步关系，再分析一个线程池内线程之间的同步关系。 单个进程不能避免锁竞争，可以通过开启多个进程来完成减少锁的影响。在这次的压测过程中，也测试了多进程网络服务的性能。多进程网络服务，可参考这篇文章。 参考 性能优化模式 【书籍】现代操作系统 jstack的工作原理 openmp Guide into OpenMP","link":"/2019/12/29/%E7%BA%BF%E4%B8%8A%E6%9C%8D%E5%8A%A1%E5%B9%B6%E5%8F%91%E4%BC%98%E5%8C%96/"},{"title":"面试（二）","text":"okcoin 程序优化方案为什么有效（整个一面都是在讨论这个） netty线程模型 nginx路由转发（rpc不能通过nginx转发） 多进程监听同一个端口有什么用 flink state如何存储，增量存储 flink checkpoint原理 cms与G1的区别，分别阐述优缺点 为什么换工作，想找什么样的工作，有什么业绩，这个业务为什么不招一个更有经验的 美团 hbase rowkey如何设计 hbase存储方案 redis集群一致性，引申：fusion，hbase，hdfs如何保证副本一致性 kafka如何扩容 kafka高性能原理 zookeeper选举过程 cms触发full gc方式，垃圾回收算法，（cms与g1区别） synchronized原理，与ReentrantLock区别 AQS原理 ConcurrentHashMap原理，分段和非分段的两种实现方式 feed流如何设计 缓存击穿 mysql分库分表 mysql分表后主键如何设计 循环链表","link":"/2018/02/16/%E9%9D%A2%E8%AF%95%E4%BA%8C/"},{"title":"面试（一）","text":"美团外卖 Hashmap原理 一道设计题，如何使用面向对象设计电梯控制程序 循环链表 美团出行 Tcp粘包，为啥会有粘包问题 Es的优化问题 为啥要换工作 快手 Mysql索引原理，优化 一道桶排序问题 汉诺塔 程序热重启 阿里 为啥要使用redis做缓存 台阶问题，时间复杂度 小米 切面原理 顺时针打印数组，通过坐标系可找出规律 动态规划中的计算矩阵路径条数问题 腾讯 链表是否相交 进程间通信方式 Boss直聘 Jvm内存分布 触发full gc的方式 同时调用多个服务，返回最先获取的结果 ThreadLocal volatile 为什么换工作，想找份什么样的工作","link":"/2018/02/16/%E9%9D%A2%E8%AF%95%E4%B8%80/"},{"title":"网站镜像工具","text":"项目地址 简介将一个网站的镜像到本地，主要用于学习和提升，涉及到python，数据库，多线程，锁，字符编码，http规范等知识点。目前仅在linux平台测试运行过。 功能： 本程序目前支持断点下载，就是如果程序在运行中意外终止，重新运行就可以继续恢复到之前状态，不用重新再去运行程序。 支持多配置文件，通过在程序运行时指定不同的配置文件，就可以通过运行多个不同的任务并行下载. 通过自定义线程池，可达到在所有链接访问一遍之后，自动停止运行并推出。 编码自适应，通过解析response响应头的数据和网页中的meta信息来筛选出最符合当前网页的编码 不仅能够解析出html中的url，同时也支持解析css中的URL 可指定运行目录，如果指定运行目录，在数据和日志就会输出到指定目录下 环境配置 系统：Linux 或 Mac OS 数据库: mysql Python3, pip3, 开发环境是3.6.4 使用方式 首先需要初始化环境，通过运行 bin/init.sh脚本来初始化环境, 目的是创建mysql数据库和表，初始化python3虚拟环境 配置文件, 主要关注site::key, site::domain, site::start_urls, site::thread_cnt, log::path, mysql::*, 这几个配置具体含义在示例中都有说明。 运行方式, bash bin/mirror.sh, 如果不加参数则使用 config/conf.ini配置. bash bin/mirror.sh -c /tmp/conf.ini表示使用/tmp/conf.ini配置文件 bash bin/mirror.sh -c &quot;/tmp/conf-dygang.ini&quot; bash bin/mirror.sh -d &quot;/tmp/&quot; 表示/tmp/作为执行目录，日志和下载的数据会存储到这个目录下，并且会优先从这个目录下/tmp/config寻找配置文件 bash bin/mirror.sh -d &quot;/tmp/&quot; -c &quot;/tmp1/conf-test.ini&quot; 表示/tmp/作为执行目录，日志和下载的数据会存储到这个目录下，但是会优先使用/tmp1/conf-test.ini 但是指定本次程序中加载的配置文件*.ini其中如果配置日志和数据路径为绝对路径，则数据会存储到绝对路径下。 bash bin/miror.sh -h 查看使用帮助","link":"/2018/07/29/%E7%BD%91%E7%AB%99%E9%95%9C%E5%83%8F%E5%B7%A5%E5%85%B7/"},{"title":"记一次连接池错误","text":"背景最近有一个服务出现了特别诡异的问题，在并发高的时候，客户端线程T-A发出请求Req-A，线程T-B发出请求Req-B，最后可能是T-B收到响应Res-A。在并发很低的时候，不会出现这个问题。 服务端和客户端信息如下: 服务端: 语言：java 协议：thrift 服务：THsHaServer 部署环境：docker 高峰QPS：3w客户端： 语言：java 部署环境：docker 访问方式：连接池来保持多个长连接 并发：多线程问题排查当时发现了返回的结果和预期不符合之后，首先认为是服务端的问题，通过打印（入参，执行结果）来验证服务端逻辑是否正确，但是发现服务端计算逻辑没有任何问题。 然后客户端打印（请求参数，结果），写了一个python脚本统计了一下结果，发现了开头的现象。这就很诡异了，一时没有什么思路。 我们组另外一位大哥就逐步浏览代码了，然后发现请求在发生超时异常时，并没有将这个链接销毁，觉得问题可能是这里导致的，然后验证了下，果然是的。。。 我们通过下图看下这是如何导致问题发生的。 t1时刻发送了ReqA t2时刻由于ReqA超时，而放弃了tcp连接，但这个时候还响应A还没被客户端接收到 t3时刻另外一个线程从连接池里获取了这个被放弃的tcp连接，发送出请求ReqB t4时刻ResA先于ResB被client接收到，问题发生 t5时刻ResB回到家，发现物是人非 当时没有根据问题的现象来往这方面想，是因为对tcp全双工和半双工不熟悉导致的。 解决方案找到问题根源，解决起来就非常简单了。在等待结果的时候，如果发生异常，就销毁这个连接。 引申这个问题还能再引申到tcp的全双工和半双工问题上。 全双工指的是同一个tcp连接其中的任何一端均可以同时的发送请求和接收结果，且不会发生本篇文章中的问题。http2使用的是这个协议。 半双工指的是tcp连接在任何一个时刻，其中一端在发出请求之后，必须等到结果之后才能再重新发送请求。大部分的网络协议均使用的是半双工。","link":"/2018/11/06/%E8%AE%B0%E4%B8%80%E6%AC%A1%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%94%99%E8%AF%AF/"},{"title":"面试（三）","text":"这是几个开放性问题，不同的阶段，思考这几个问题的方式也是不一样的。 为什么想换工作 期望的岗位是什么样的你的产出是什么，亮点在哪你对我们公司怎么看为什么让你负责这个业务","link":"/2018/02/22/%E9%9D%A2%E8%AF%95%E4%B8%89/"}],"tags":[{"name":"Flink源码","slug":"Flink源码","link":"/tags/Flink%E6%BA%90%E7%A0%81/"},{"name":"OLAP","slug":"OLAP","link":"/tags/OLAP/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"笔记","slug":"笔记","link":"/tags/%E7%AC%94%E8%AE%B0/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"Flink","slug":"Flink","link":"/tags/Flink/"},{"name":"tcp","slug":"tcp","link":"/tags/tcp/"},{"name":"网络","slug":"网络","link":"/tags/%E7%BD%91%E7%BB%9C/"},{"name":"生活小技巧","slug":"生活小技巧","link":"/tags/%E7%94%9F%E6%B4%BB%E5%B0%8F%E6%8A%80%E5%B7%A7/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/tags/ZooKeeper/"},{"name":"方法论","slug":"方法论","link":"/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"案例","slug":"案例","link":"/tags/%E6%A1%88%E4%BE%8B/"},{"name":"算法题","slug":"算法题","link":"/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"},{"name":"微服务","slug":"微服务","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"一致性","slug":"一致性","link":"/tags/%E4%B8%80%E8%87%B4%E6%80%A7/"},{"name":"并发","slug":"并发","link":"/tags/%E5%B9%B6%E5%8F%91/"},{"name":"高性能","slug":"高性能","link":"/tags/%E9%AB%98%E6%80%A7%E8%83%BD/"},{"name":"特征平台","slug":"特征平台","link":"/tags/%E7%89%B9%E5%BE%81%E5%B9%B3%E5%8F%B0/"},{"name":"数据一致性","slug":"数据一致性","link":"/tags/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/"},{"name":"流处理","slug":"流处理","link":"/tags/%E6%B5%81%E5%A4%84%E7%90%86/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"异步","slug":"异步","link":"/tags/%E5%BC%82%E6%AD%A5/"},{"name":"面试","slug":"面试","link":"/tags/%E9%9D%A2%E8%AF%95/"},{"name":"美团","slug":"美团","link":"/tags/%E7%BE%8E%E5%9B%A2/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"杂谈","slug":"杂谈","link":"/categories/%E6%9D%82%E8%B0%88/"},{"name":"计算","slug":"分布式/计算","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E8%AE%A1%E7%AE%97/"},{"name":"工程技术","slug":"工程技术","link":"/categories/%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF/"},{"name":"架构","slug":"工程技术/架构","link":"/categories/%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF/%E6%9E%B6%E6%9E%84/"},{"name":"总结","slug":"工程技术/总结","link":"/categories/%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF/%E6%80%BB%E7%BB%93/"},{"name":"项目","slug":"工程技术/项目","link":"/categories/%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/"},{"name":"思考","slug":"工程技术/思考","link":"/categories/%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF/%E6%80%9D%E8%80%83/"}]}